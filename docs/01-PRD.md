Product Requirements Document: Foundry
Generated by: Product Definition Agent v17 (Application-Agnostic)
Date: 2026-01-21
Status: Draft

1. Executive Summary
Product Overview
Foundry is a multi-tenant SaaS platform that transforms raw business data from heterogeneous sources into clean, privacy-compliant, structured datasets optimized for AI systems, agents, and evaluation workflows. The platform operates as a configuration-driven preparation layer between operational systems and AI tooling, enabling organizations to safely unlock the value of their existing data without custom engineering.
Value Proposition
Foundry eliminates the technical barrier between "data in systems" and "AI-ready datasets" by providing a universal, configurable, privacy-first platform that works across any data source, industry, or AI use case.
Target Market

Primary: B2B organizations adopting AI agents, RAG systems, or internal AI tools (50-1000 employees)
Secondary: AI service providers and consultancies preparing client data
Initial focus: Companies with customer support operations, sales teams, and internal knowledge management needs

Key Differentiators

Source-agnostic architecture treats all inputs (files, APIs, databases) equally
Configuration-driven workflows enable non-technical users to prepare datasets
Privacy and de-identification as first-class features with audit trails
Schema-first normalization ensures AI systems receive predictable, consistent data
Incremental value from single file upload to multi-source automation

Deployment Target
Replit (web application)

2. Problem Statement
The Problem
Organizations adopting AI agents and intelligent systems face a critical bottleneck: transforming operational business data into AI-ready datasets. B2B companies with 50-1000 employees typically operate 8-15 different systems (CRM, helpdesk, documentation, spreadsheets, internal tools), each with unique data structures and privacy considerations.
A mid-sized software company launching a customer support AI agent spends 120-200 hours of engineering time extracting tickets from their helpdesk, manually redacting customer PII, normalizing conversation formats, and converting data into training-compatible structures. This process must be repeated for every new data source, every AI initiative, and whenever source data changes. The technical complexity, privacy risk, and time investment prevent most organizations from leveraging their most valuable asset: their own operational data.
Current Alternatives
Manual CSV Export + Spreadsheet Manipulation

Limitations: Time-intensive (40+ hours per dataset), error-prone, no PII handling, breaks when schemas change, doesn't scale beyond small datasets

Custom Python Scripts per Source

Limitations: Requires engineering resources for every source and use case, fragile to API changes, no governance layer, difficult to maintain across multiple projects

Data Warehouse + ETL Tools

Limitations: Designed for analytics not AI, over-engineered for preparation tasks, expensive, steep learning curve, still requires custom transformation logic

AI Platform "Data Prep" Features

Limitations: Vendor lock-in, limited to platform's data formats, minimal privacy controls, source connectivity gaps

Quantified Impact

Engineering Time: Teams spend 80-200 hours per AI project preparing data across sources
Privacy Risk: 73% of support tickets contain customer PII that must be manually redacted
Dataset Staleness: Manually prepared datasets become outdated within 30-90 days but require same effort to refresh
Project Abandonment: 40-60% of internal AI projects stall during data preparation phase due to complexity
Compliance Exposure: Ad-hoc de-identification lacks audit trails required for GDPR/SOC2

Who Experiences This Pain

Product Managers launching AI-powered features who lack engineering resources for data preparation
Operations Leaders wanting to leverage existing process data for AI assistants but blocked by privacy concerns
AI/ML Engineers spending 60-70% of project time on data extraction and cleaning instead of model work
Compliance Officers unable to verify how customer data is being used for AI training


3. User Personas
Persona 1: Sarah Chen - Product Manager (Primary)
Demographics:

Job Title: Senior Product Manager
Company Size: 150-500 employees (B2B SaaS)
Industry: Software, Professional Services
Technical Proficiency: Intermediate (comfortable with SaaS tools, no coding)

Goals:

Launch AI-powered support agent to reduce ticket volume by 30%
Use real customer conversations to train agent on company-specific knowledge
Prove value of AI investment with clean datasets from existing systems
Deliver AI features without waiting 6-12 weeks for engineering resources

Motivations:

Career advancement through successful AI product launches
Reduce dependency on engineering for data tasks
Demonstrate ROI on AI initiatives to leadership
Enable faster iteration and experimentation

Pain Points:

Can't extract data from helpdesk without engineering help
Worried about accidentally exposing customer PII in training data
Previous AI project delayed 8 weeks waiting for data engineering
Struggles to explain to engineering exactly what data structure AI vendor needs
Can't refresh datasets when conversations change without repeating entire process

Usage Context:
Works from laptop during business hours, manages 2-3 concurrent product initiatives, needs to show progress weekly, collaborates with external AI vendors who require specific data formats
Success Metrics (From Their Perspective):

Can prepare dataset in under 1 day vs. 4-6 weeks
Zero PII exposure incidents in AI training data
Can regenerate datasets monthly as support patterns evolve
Engineering team spends zero hours on data prep tasks


Persona 2: Marcus Rodriguez - AI/ML Engineer (Primary)
Demographics:

Job Title: Machine Learning Engineer
Company Size: 100-800 employees
Industry: Technology, Financial Services, Healthcare
Technical Proficiency: Advanced (Python, APIs, data pipelines)

Goals:

Build internal AI tools using company's operational data
Ensure training datasets are clean, consistent, and compliant
Reduce time spent on data wrangling from 70% to under 30% of project time
Create reusable data pipelines that non-technical teams can operate

Motivations:

Focus on ML problems instead of data engineering
Eliminate repetitive extraction and cleaning work
Ship AI projects faster to demonstrate team value
Reduce technical debt from one-off data scripts

Pain Points:

Every data source requires custom integration code (Zendesk, Salesforce, Google Sheets)
Conversation data from different sources has incompatible schemas
Manually identifying and redacting PII in 10,000+ records is error-prone
Stakeholders request dataset refreshes but rerunning scripts takes days
No audit trail showing how training data was transformed

Usage Context:
Uses platform programmatically via API when available, needs to explain data lineage to compliance, manages 3-5 AI projects simultaneously, works with product and ops teams who need self-service access
Success Metrics (From Their Perspective):

Integrate new data source in hours not weeks
Zero custom code for standard transformations (PII, normalization)
Dataset provenance automatically documented
Non-engineers can refresh datasets without his involvement


Persona 3: Jennifer Park - Director of Customer Success (Secondary)
Demographics:

Job Title: VP/Director of Customer Success
Company Size: 200-1000 employees
Industry: B2B SaaS, Professional Services
Technical Proficiency: Beginner-Intermediate (Excel power user, no technical tools)

Goals:

Use support ticket history to train AI assistant for team efficiency
Identify patterns in customer issues to inform product improvements
Ensure customer data privacy when adopting AI tools
Demonstrate measurable impact of support operations (resolution time, quality)

Motivations:

Scale support team efficiency without proportional headcount growth
Competitive pressure to adopt AI in customer-facing roles
Protect customer trust through responsible data handling
Quantify team performance with data-driven insights

Pain Points:

Helpdesk data contains sensitive customer information (names, emails, account details)
Depends on IT/engineering to export and clean data for AI vendors
Can't experiment with different AI tools because data prep is too expensive
Worried about compliance violations if PII leaks into training data
No visibility into how raw tickets become training datasets

Usage Context:
Manages team of 8-25 support agents, reports to COO/CEO, evaluates AI vendors quarterly, needs to show ROI on operational investments, works primarily from web browser
Success Metrics (From Their Perspective):

Can prepare compliance-safe dataset for AI vendor evaluation in 48 hours
Customer PII automatically removed with audit trail for compliance review
Team can refresh datasets monthly without IT involvement
Confidence that no raw customer data is exposed


Persona 4: David Kim - Compliance Officer (Secondary)
Demographics:

Job Title: Head of Compliance / Data Protection Officer
Company Size: 300-2000 employees
Industry: Financial Services, Healthcare, Enterprise SaaS
Technical Proficiency: Intermediate (understands data governance, not a developer)

Goals:

Ensure AI training data complies with GDPR, CCPA, SOC2, HIPAA
Maintain audit trails showing how customer data is processed
Prevent PII exposure in AI models and training datasets
Enable business to use data for AI without introducing compliance risk

Motivations:

Protect company from regulatory fines and customer trust violations
Balance enabling innovation with maintaining governance
Demonstrate due diligence to auditors and regulators
Reduce manual review burden for every AI initiative

Pain Points:

Engineering teams build custom data pipelines with no governance controls
No centralized view of what data is used for which AI projects
Ad-hoc PII redaction lacks consistent standards and documentation
Can't verify compliance when data prep happens in scripts or notebooks
Audit trail gaps make SOC2/GDPR certification difficult

Usage Context:
Reviews data processing activities quarterly, responds to data subject access requests, works with legal and engineering, requires documentation for audits
Success Metrics (From Their Perspective):

Complete audit trail from source data to AI-ready output
Consistent PII detection and masking policies across all projects
Can generate compliance report showing data lineage in under 1 hour
Zero customer data breaches related to AI training activities


4. User Stories and Requirements
User Story Template
ID: US-[Category]-[Number]
Persona: [Persona Name]
Story: As a [persona], I want to [action] so that [benefit]

Acceptance Criteria:
  - Given [context], when [action], then [outcome]
  - Given [context], when [action], then [outcome]

Priority: [P0-Critical | P1-High | P2-Medium | P3-Low]
MVP Status: [MVP | Post-MVP | Future]
Dependencies: [US-XXX, US-YYY]
Estimated Complexity: [S | M | L | XL]
Authentication Stories
US-AUTH-001: User Registration via Invitation
Persona: Sarah Chen (Product Manager)
Story: As a new user, I want to register using an invitation link so that I can join my organization's Foundry account
Acceptance Criteria:

Given I receive an invitation email with token, when I click the link, then I'm directed to registration page with organization pre-filled
Given I'm on registration page, when I enter name and password, then my account is created and I'm logged in
Given invitation has expired (>7 days), when I click link, then I see "invitation expired" message with option to request new invite

Priority: P0-Critical
MVP Status: MVP
Dependencies: None
Estimated Complexity: M
US-AUTH-002: User Login
Persona: All Personas
Story: As a registered user, I want to log in with email and password so that I can access my organization's projects
Acceptance Criteria:

Given I have valid credentials, when I submit login form, then I'm redirected to projects dashboard
Given I enter incorrect password, when I submit, then I see "Invalid email or password" error
Given I haven't logged in for 30 days, when I log in, then my session remains valid for 24 hours

Priority: P0-Critical
MVP Status: MVP
Dependencies: US-AUTH-001
Estimated Complexity: S
US-AUTH-003: Password Reset
Persona: All Personas
Story: As a user who forgot my password, I want to reset it via email so that I can regain access to my account
Acceptance Criteria:

Given I'm on login page, when I click "Forgot password", then I'm directed to password reset request page
Given I enter my email, when I submit, then I receive reset link within 5 minutes
Given I click reset link, when I enter new password, then my password is updated and I can log in

Priority: P1-High
MVP Status: MVP
Dependencies: US-AUTH-002
Estimated Complexity: M
US-AUTH-004: Team Member Invitation
Persona: Sarah Chen (Product Manager)
Story: As an organization admin, I want to invite team members so that they can collaborate on data preparation projects
Acceptance Criteria:

Given I'm on team settings page, when I enter colleague's email and select role, then invitation email is sent
Given invitation is sent, when recipient clicks link within 7 days, then they can register
Given invitation expires, when I check invitations list, then I can resend invitation

Priority: P1-High
MVP Status: MVP
Dependencies: US-AUTH-001
Estimated Complexity: M
Organization Management Stories
US-ORG-001: Organization Creation
Persona: Sarah Chen (Product Manager)
Story: As a first user, I want my organization to be automatically created during registration so that I can immediately start using Foundry
Acceptance Criteria:

Given I register without invitation, when registration completes, then new organization is created with my company name
Given organization is created, when I log in, then I'm assigned admin role
Given I'm admin, when I access organization settings, then I can modify organization name

Priority: P0-Critical
MVP Status: MVP
Dependencies: US-AUTH-001
Estimated Complexity: M
US-ORG-002: Team Member Management
Persona: Jennifer Park (Director of Customer Success)
Story: As an organization admin, I want to view all team members and their roles so that I can manage access
Acceptance Criteria:

Given I'm on team page, when page loads, then I see list of all users with name, email, role, and status
Given I select a member, when I change their role, then their permissions update immediately
Given I select inactive member, when I click "Remove", then they lose access and are removed from list

Priority: P1-High
MVP Status: MVP
Dependencies: US-AUTH-004
Estimated Complexity: M
Project Management Stories
US-PROJ-001: Create New Project
Persona: Sarah Chen (Product Manager)
Story: As a user, I want to create a project so that I can prepare a dataset for a specific AI initiative
Acceptance Criteria:

Given I'm on projects dashboard, when I click "New Project", then I see project creation form
Given I enter project name and description, when I click "Create", then project is created and I'm directed to project page
Given project is created, when I view projects list, then new project appears with status "Empty"

Priority: P0-Critical
MVP Status: MVP
Dependencies: US-ORG-001
Estimated Complexity: S
US-PROJ-002: View Projects List
Persona: Marcus Rodriguez (AI/ML Engineer)
Story: As a user, I want to see all projects in my organization so that I can navigate to relevant work
Acceptance Criteria:

Given I'm logged in, when I navigate to dashboard, then I see list of all projects with name, owner, last updated, and status
Given projects list loads, when I click a project, then I'm directed to that project's detail page
Given I have no projects, when page loads, then I see empty state with "Create your first project" call-to-action

Priority: P0-Critical
MVP Status: MVP
Dependencies: US-PROJ-001
Estimated Complexity: S
US-PROJ-003: Edit Project Details
Persona: Sarah Chen (Product Manager)
Story: As a project owner, I want to edit project name and description so that I can keep project information accurate
Acceptance Criteria:

Given I'm on project page, when I click "Edit", then I see editable form for name and description
Given I modify fields, when I click "Save", then changes are persisted and displayed
Given I haven't made changes, when I click "Cancel", then form closes without saving

Priority: P2-Medium
MVP Status: MVP
Dependencies: US-PROJ-001
Estimated Complexity: XS
US-PROJ-004: Delete Project
Persona: Marcus Rodriguez (AI/ML Engineer)
Story: As a project owner, I want to delete projects I no longer need so that I can keep my workspace organized
Acceptance Criteria:

Given I'm on project page, when I click "Delete" and confirm, then project and all associated data are removed
Given I delete project, when I return to dashboard, then deleted project no longer appears
Given project contains processed datasets, when I delete, then I'm warned that all outputs will be lost

Priority: P2-Medium
MVP Status: Post-MVP
Dependencies: US-PROJ-001
Estimated Complexity: S
Data Source Management Stories
US-SRC-001: Upload File as Data Source
Persona: Sarah Chen (Product Manager)
Story: As a user, I want to upload a CSV file so that I can use it as a data source for my project
Acceptance Criteria:

Given I'm on project page, when I click "Upload File", then I see file upload dialog
Given I select CSV file (<100MB), when upload completes, then file appears in project sources with "Ready" status
Given upload fails, when error occurs, then I see specific error message explaining the issue

Priority: P0-Critical
MVP Status: MVP
Dependencies: US-PROJ-001
Estimated Complexity: M
US-SRC-002: Upload Excel File as Data Source
Persona: Jennifer Park (Director of Customer Success)
Story: As a user, I want to upload an Excel file so that I can use spreadsheet exports from operational systems
Acceptance Criteria:

Given I'm on project page, when I upload .xlsx file, then I can select which sheet to use as data source
Given Excel has multiple sheets, when I upload, then I see sheet selection dialog
Given I select sheet, when I confirm, then data is imported and available for configuration

Priority: P1-High
MVP Status: MVP
Dependencies: US-SRC-001
Estimated Complexity: M
US-SRC-003: Upload JSON File as Data Source
Persona: Marcus Rodriguez (AI/ML Engineer)
Story: As a technical user, I want to upload JSON files so that I can use API exports and structured data exports
Acceptance Criteria:

Given I upload JSON file, when file is valid JSON, then system detects structure and displays preview
Given JSON is array of objects, when I confirm, then each object becomes a record
Given JSON is malformed, when upload completes, then I see validation error with line number

Priority: P1-High
MVP Status: MVP
Dependencies: US-SRC-001
Estimated Complexity: M
US-SRC-004: Connect Teamwork Desk Account
Persona: Sarah Chen (Product Manager)
Story: As a user, I want to connect my Teamwork Desk account so that I can import support tickets automatically
Acceptance Criteria:

Given I'm on project page, when I click "Connect API", then I see list of available connectors with Teamwork Desk
Given I select Teamwork Desk, when I enter API credentials, then system validates connection
Given connection succeeds, when I click "Authorize", then Teamwork Desk appears in project sources

Priority: P0-Critical
MVP Status: MVP
Dependencies: US-PROJ-001
Estimated Complexity: L
US-SRC-005: View Data Source Preview
Persona: Marcus Rodriguez (AI/ML Engineer)
Story: As a user, I want to preview source data so that I can verify correct data was imported
Acceptance Criteria:

Given source is uploaded, when I click source name, then I see first 100 rows in table view
Given I'm viewing preview, when I scroll, then additional rows load
Given source has many columns, when I view preview, then I can horizontally scroll to see all fields

Priority: P1-High
MVP Status: MVP
Dependencies: US-SRC-001
Estimated Complexity: S
Data Configuration Stories
US-CFG-001: Auto-Detect Schema from Source
Persona: Sarah Chen (Product Manager)
Story: As a non-technical user, I want the system to automatically detect data structure so that I don't have to manually configure every field
Acceptance Criteria:

Given I upload CSV with headers, when upload completes, then system displays detected columns and types
Given columns are detected, when I view configuration, then suggested field mappings are pre-filled
Given detection is wrong, when I manually adjust, then my changes override auto-detection

Priority: P0-Critical
MVP Status: MVP
Dependencies: US-SRC-001
Estimated Complexity: M
US-CFG-002: Map Source Fields to Canonical Schema
Persona: Marcus Rodriguez (AI/ML Engineer)
Story: As a user, I want to map my source columns to standard conversation schema so that output is structured consistently
Acceptance Criteria:

Given I have uploaded source, when I click "Configure Mapping", then I see source fields on left and canonical schema on right
Given I select source field, when I drag to schema field, then mapping is created
Given mapping is incomplete, when I try to process, then I see which required fields are missing

Priority: P0-Critical
MVP Status: MVP
Dependencies: US-CFG-001
Estimated Complexity: L
US-CFG-003: Configure PII Detection Rules
Persona: David Kim (Compliance Officer)
Story: As a compliance user, I want to configure which data types should be detected as PII so that de-identification meets our policies
Acceptance Criteria:

Given I'm in project settings, when I navigate to "Privacy Rules", then I see list of PII types (email, phone, names, etc.)
Given I select PII types, when I enable/disable, then detection rules apply to all processing
Given I have custom PII patterns (account IDs), when I add regex rule, then custom pattern is detected

Priority: P1-High
MVP Status: MVP
Dependencies: US-CFG-002
Estimated Complexity: M
US-CFG-004: Configure De-identification Strategy
Persona: Jennifer Park (Director of Customer Success)
Story: As a user concerned with privacy, I want to choose how PII is masked so that datasets remain useful while protecting privacy
Acceptance Criteria:

Given PII is detected, when I configure masking, then I can choose strategy per type (redact, hash, pseudonymize)
Given I select "Redact", when processing runs, then PII is replaced with [REDACTED]
Given I select "Pseudonymize", when processing runs, then consistent fake values replace originals (same email always becomes same pseudonym)

Priority: P1-High
MVP Status: MVP
Dependencies: US-CFG-003
Estimated Complexity: L
US-CFG-005: Configure Data Filters
Persona: Marcus Rodriguez (AI/ML Engineer)
Story: As a user, I want to filter which records are included in output so that I can focus on relevant, high-quality data
Acceptance Criteria:

Given I'm configuring project, when I add filter rule, then I can specify conditions (e.g., "status = closed")
Given multiple filters exist, when I save, then only records matching all conditions are included in output
Given I preview dataset, when filters are applied, then I see record count before and after filtering

Priority: P1-High
MVP Status: MVP
Dependencies: US-CFG-002
Estimated Complexity: M
Data Processing Stories
US-PROC-001: Trigger Dataset Processing
Persona: Sarah Chen (Product Manager)
Story: As a user, I want to manually trigger dataset processing so that I can generate AI-ready output on demand
Acceptance Criteria:

Given project is configured, when I click "Process Dataset", then processing job starts
Given processing starts, when I view project page, then I see progress indicator with status
Given processing completes, when I return to project, then I see "Completed" status with timestamp

Priority: P0-Critical
MVP Status: MVP
Dependencies: US-CFG-002
Estimated Complexity: L
US-PROC-002: Monitor Processing Progress
Persona: Marcus Rodriguez (AI/ML Engineer)
Story: As a user, I want to see processing progress so that I know when dataset will be ready
Acceptance Criteria:

Given processing is running, when I view project page, then I see progress percentage and estimated time remaining
Given processing fails, when error occurs, then I see error message with details
Given processing completes, when I refresh page, then status updates to "Ready for Download"

Priority: P1-High
MVP Status: MVP
Dependencies: US-PROC-001
Estimated Complexity: M
US-PROC-003: View Processing Statistics
Persona: Jennifer Park (Director of Customer Success)
Story: As a user, I want to see processing statistics so that I can understand what transformations were applied
Acceptance Criteria:

Given processing completes, when I view results, then I see total records processed, PII instances detected, records filtered out
Given PII was detected, when I view statistics, then I see breakdown by type (emails, phones, names)
Given records were filtered, when I view statistics, then I see which filter rules removed which percentage

Priority: P1-High
MVP Status: MVP
Dependencies: US-PROC-001
Estimated Complexity: M
US-PROC-004: Re-run Processing with Updated Configuration
Persona: Marcus Rodriguez (AI/ML Engineer)
Story: As a user, I want to re-process dataset after changing configuration so that I can iterate on output quality
Acceptance Criteria:

Given processing completed previously, when I modify configuration and click "Re-process", then new job starts
Given new job starts, when it completes, then previous output is replaced with new version
Given I re-process, when I view history, then I can see what changed between runs

Priority: P2-Medium
MVP Status: Post-MVP
Dependencies: US-PROC-001
Estimated Complexity: M
Dataset Export Stories
US-EXP-001: Download Processed Dataset as JSONL
Persona: Marcus Rodriguez (AI/ML Engineer)
Story: As a technical user, I want to download dataset as JSONL so that I can use it for model fine-tuning
Acceptance Criteria:

Given processing completed, when I click "Download", then I see format options including JSONL
Given I select JSONL, when download starts, then file contains one JSON object per line
Given dataset is large, when download completes, then file is compressed (.jsonl.gz)

Priority: P0-Critical
MVP Status: MVP
Dependencies: US-PROC-001
Estimated Complexity: M
US-EXP-002: Download Processed Dataset as CSV
Persona: Sarah Chen (Product Manager)
Story: As a non-technical user, I want to download dataset as CSV so that I can review output in spreadsheet tools
Acceptance Criteria:

Given processing completed, when I select CSV format, then download contains all fields as columns
Given dataset has nested fields, when exported to CSV, then nested data is flattened with dot notation
Given I open CSV, when I view in Excel, then all data displays correctly

Priority: P1-High
MVP Status: MVP
Dependencies: US-PROC-001
Estimated Complexity: M
US-EXP-003: Download Processed Dataset as Q&A Pairs JSON
Persona: Sarah Chen (Product Manager)
Story: As a user preparing data for RAG systems, I want to download as Q&A pairs so that data is in format expected by AI vendors
Acceptance Criteria:

Given source is conversational, when I select "Q&A Pairs" format, then output contains question-answer pairs
Given conversation has multiple exchanges, when exported, then each exchange becomes separate Q&A pair
Given I open output, when I view JSON, then structure matches common RAG format: [{question: "", answer: ""}]

Priority: P1-High
MVP Status: MVP
Dependencies: US-PROC-001
Estimated Complexity: M
US-EXP-004: View Data Lineage Report
Persona: David Kim (Compliance Officer)
Story: As a compliance user, I want to see data lineage showing source-to-output transformations so that I can document processing for audits
Acceptance Criteria:

Given processing completed, when I click "View Lineage", then I see report listing source files, transformations applied, PII masked
Given lineage report opens, when I scroll, then I see each processing stage with timestamp and configuration
Given I need audit trail, when I click "Export Report", then PDF is generated with complete lineage

Priority: P1-High
MVP Status: MVP
Dependencies: US-PROC-001
Estimated Complexity: L
Teamwork Desk Integration Stories
US-TWD-001: Authenticate with Teamwork Desk
Persona: Sarah Chen (Product Manager)
Story: As a user, I want to connect my Teamwork Desk account so that I can automatically import support tickets
Acceptance Criteria:

Given I'm on source configuration, when I select Teamwork Desk connector, then I see authentication form
Given I enter Teamwork Desk API domain and key, when I click "Connect", then system validates credentials
Given credentials are valid, when connection succeeds, then I see "Connected" status with domain name

Priority: P0-Critical
MVP Status: MVP
Dependencies: US-PROJ-001
Estimated Complexity: M
US-TWD-002: Configure Ticket Import Filters
Persona: Jennifer Park (Director of Customer Success)
Story: As a user, I want to specify which tickets to import so that I only process relevant conversations
Acceptance Criteria:

Given Teamwork Desk is connected, when I configure import, then I can filter by date range, status, inbox, tags
Given I set date range, when I click "Preview", then I see estimated ticket count matching filters
Given I set status filter to "Closed", when import runs, then only closed tickets are imported

Priority: P0-Critical
MVP Status: MVP
Dependencies: US-TWD-001
Estimated Complexity: M
US-TWD-003: Import Ticket Conversations
Persona: Sarah Chen (Product Manager)
Story: As a user, I want to import full ticket conversations including all messages so that I capture complete context
Acceptance Criteria:

Given filters are configured, when I trigger import, then system fetches tickets matching criteria
Given import completes, when I preview data, then I see customer messages and agent responses as separate entries
Given ticket has 10 messages, when imported, then all 10 messages appear in chronological order

Priority: P0-Critical
MVP Status: MVP
Dependencies: US-TWD-002
Estimated Complexity: L
US-TWD-004: Identify Customer vs. Agent Messages
Persona: Marcus Rodriguez (AI/ML Engineer)
Story: As a user preparing training data, I want to automatically distinguish customer messages from agent responses so that I can structure data correctly
Acceptance Criteria:

Given ticket is imported, when system processes messages, then each message is tagged as "customer" or "agent"
Given I configure schema mapping, when I map to canonical format, then role field is automatically populated
Given I export dataset, when I view output, then role distinction is preserved in final structure

Priority: P0-Critical
MVP Status: MVP
Dependencies: US-TWD-003
Estimated Complexity: M
US-TWD-005: Extract Ticket Metadata
Persona: Sarah Chen (Product Manager)
Story: As a user, I want to include ticket metadata (status, priority, tags) so that I can filter or enrich datasets
Acceptance Criteria:

Given ticket is imported, when I view source preview, then I see metadata fields (status, priority, assignee, tags, created_at, closed_at)
Given I configure mapping, when I map metadata fields, then they are available in output schema
Given I filter by metadata, when I set "priority = high", then only high-priority tickets are included

Priority: P1-High
MVP Status: MVP
Dependencies: US-TWD-003
Estimated Complexity: M
Story Summary
PriorityCount% of TotalP0-Critical1539%P1-High1950%P2-Medium38%P3-Low13%Total38100%
MVP StatusCount% of TotalMVP3592%Post-MVP38%Future00%Total38100%

5. Feature Specification
Feature Template
ID: FEAT-[Number]
Name: [Feature Name]
User Stories: [US-XXX, US-YYY]

Description:
[Detailed description of what the feature does]

Functional Requirements:
- [Requirement 1]
- [Requirement 2]

Non-Functional Requirements:
- Performance: [Expected performance criteria]
- Security: [Security requirements]
- Accessibility: [WCAG level, specific needs]

Edge Cases:
- [Edge case 1 and how to handle]
- [Edge case 2 and how to handle]

Error States:
- [Error scenario 1 and user feedback]
- [Error scenario 2 and user feedback]

Out of Scope (Post-MVP):
- [What's explicitly NOT included in MVP]
FEAT-001: Authentication and Authorization
User Stories: US-AUTH-001, US-AUTH-002, US-AUTH-003, US-AUTH-004
Description:
Multi-tenant authentication system supporting email/password login, invitation-based registration, password reset, and session management. Each user belongs to one organization with role-based permissions (admin or member).
Functional Requirements:

Email/password authentication with JWT tokens (24h expiry per Constitution)
Invitation-based registration with 7-day token expiry
Password reset via email with 1-hour token expiry
Role-based access control: admin (all actions) and member (view/use projects)
Organization isolation: users can only access their organization's data
Session persistence across browser closes for 24 hours

Non-Functional Requirements:

Performance: Login response <500ms, password reset email delivery <2 minutes
Security: bcrypt password hashing (cost factor 10), JWT tokens stored in localStorage, HTTPS-only cookies
Accessibility: WCAG 2.1 AA, keyboard-accessible auth forms, screen reader support

Edge Cases:

User receives multiple invitations: Last invitation sent is valid, previous are invalidated
User clicks expired invitation link: Show "invitation expired" message with option to request new invite
User logs in from multiple devices: All sessions remain valid until individual logout
Organization admin invites non-existent user: Invitation email sent, user must register via that link

Error States:

Invalid credentials: "Invalid email or password" (don't specify which is wrong)
Email already registered: "An account with this email already exists"
Weak password: "Password must be at least 8 characters with 1 uppercase and 1 number"
Password reset for non-existent email: Return success (don't leak account existence)

Out of Scope (Post-MVP):

OAuth/SSO integration (Google, Microsoft)
Multi-factor authentication
Password strength meter UI
Account lockout after failed attempts
Session management dashboard


FEAT-002: Organization and Team Management
User Stories: US-ORG-001, US-ORG-002
Description:
Multi-tenant organization structure where each organization has isolated data, team members, and projects. Organization admins manage team membership and roles.
Functional Requirements:

Automatic organization creation for first user (becomes admin)
Invitation-based team member onboarding
Role assignment: admin (full access) or member (no team/billing management)
Team member list showing name, email, role, status (active/invited)
Organization name editing by admins
Member removal by admins

Non-Functional Requirements:

Performance: Team list loads <1s for organizations with up to 100 members
Security: Organization data isolation enforced at database query level, role checks on all admin actions
Accessibility: Team management interface keyboard-navigable, role changes announced to screen readers

Edge Cases:

Last admin removes themselves: Prevented with error "Organization must have at least one admin"
Admin changes own role to member: Allowed if other admins exist, blocked if sole admin
Invited user never registers: Invitation shows "Pending" status for 7 days, then expires
Organization created without name: Default name "Organization" assigned, editable immediately

Error States:

Invite email already used in organization: "This user is already a member"
Remove member who owns active projects: "Cannot remove user who owns projects. Transfer ownership first."
Non-admin attempts team management: "You don't have permission to manage team members"

Out of Scope (Post-MVP):

Multiple organizations per user
Organization transfer/merge
Custom role definitions beyond admin/member
Team activity audit logs
Organization deletion


FEAT-003: Project Management
User Stories: US-PROJ-001, US-PROJ-002, US-PROJ-003, US-PROJ-004
Description:
Project-based workspace organization where each project represents a single AI dataset preparation workflow. Projects contain sources, configuration, processing history, and outputs.
Functional Requirements:

Create project with name and description (name required, description optional)
List all organization projects with name, owner, last updated, status (empty/configured/processing/ready/failed)
Edit project name and description by owner
View project detail page showing sources, configuration, processing status, and outputs
Delete project and all associated data (post-MVP)
Project ownership: creator is owner, admin can transfer ownership

Non-Functional Requirements:

Performance: Projects list loads <1s for up to 500 projects, project detail page loads <2s
Security: Users can only see projects in their organization, only owner and admins can modify project settings
Accessibility: Project cards keyboard-navigable, status badges have accessible labels

Edge Cases:

User creates project with duplicate name: Allowed (names don't need to be unique)
Project owner leaves organization: Project ownership transfers to organization admin
User tries to delete project with processing in progress: Blocked with "Cannot delete project while processing"
Empty project with no sources: Shows empty state with "Add your first data source"

Error States:

Project name empty: "Project name is required"
Project description exceeds 500 characters: "Description must be under 500 characters"
User without permission tries to edit: "You don't have permission to edit this project"
Delete project with active exports: "Cannot delete project. Cancel downloads first."

Out of Scope (Post-MVP):

Project templates
Project duplication/cloning
Project archival (soft delete)
Project sharing outside organization
Project activity timeline


FEAT-004: File Upload Data Sources
User Stories: US-SRC-001, US-SRC-002, US-SRC-003, US-SRC-005
Description:
File-based data source ingestion supporting CSV, Excel, and JSON formats. Automatic schema detection, data preview, and validation.
Functional Requirements:

Upload CSV files with automatic header detection and column type inference
Upload Excel files (.xlsx) with sheet selection for multi-sheet workbooks
Upload JSON files with automatic structure detection (array of objects or nested objects)
File size limit: 100MB per file
Automatic virus scanning on upload (ClamAV or similar)
Data preview showing first 100 rows with infinite scroll
Column-level metadata (name, detected type, sample values, null count)
File retention: 30 days from upload, renewable if project is active

Non-Functional Requirements:

Performance: Upload 10MB file <15 seconds, preview loads <2 seconds, schema detection completes <5 seconds for 100k rows
Security: Virus scanning before storage, files stored encrypted at rest, pre-signed URLs for download (expire in 1 hour)
Accessibility: Upload interface supports drag-and-drop and file picker, preview table keyboard-navigable

Edge Cases:

CSV with inconsistent column counts: Pad missing columns with nulls, warning shown
Excel file with multiple sheets: Show sheet selector modal, user must choose one
JSON file with mixed types in array: Detect most common type, flag inconsistencies in preview
File upload interrupted: Show resume option for files >10MB
Duplicate filename: Append timestamp to filename, allow duplicate uploads

Error States:

File exceeds 100MB: "File size exceeds 100MB limit. Please split file or use API connector."
Unsupported file type: "File type .doc not supported. Supported types: CSV, XLSX, JSON"
Virus detected: "File failed security scan. Upload rejected."
Malformed JSON: "Invalid JSON format. Error at line 42: unexpected token"
Empty file: "File contains no data. Please check file and try again."

Out of Scope (Post-MVP):

Parquet, Avro, XML file formats
Direct cloud storage import (S3, Google Drive)
File upload progress tracking (implemented but basic)
File versioning (re-upload replaces previous)
Bulk upload of multiple files
File preview filtering/sorting


FEAT-005: Teamwork Desk API Integration
User Stories: US-TWD-001, US-TWD-002, US-TWD-003, US-TWD-004, US-TWD-005
Description:
OAuth-based integration with Teamwork Desk API to automatically import support tickets, conversations, and metadata for AI training data preparation.
Functional Requirements:

Authenticate with Teamwork Desk API using domain and API key
Configure import filters: date range (from/to), status (open/pending/closed/all), inboxes (multi-select), tags (multi-select)
Import ticket metadata: ID, subject, status, priority, assignee, tags, created_at, updated_at, closed_at
Import full conversation history with all messages in chronological order
Identify message role: customer vs. agent based on Teamwork Desk user type
Preview estimated import size before triggering (show ticket count and approximate data size)
Incremental import: Only fetch tickets updated since last import
Rate limiting: Respect Teamwork Desk API limits (120 requests/minute)

Non-Functional Requirements:

Performance: Import 1,000 tickets in <5 minutes, handle up to 10,000 tickets per import
Security: API credentials stored encrypted (AES-256-GCM per Constitution), tokens never logged in plaintext
Accessibility: API configuration form fully keyboard-accessible, connection status clearly announced

Edge Cases:

Teamwork Desk API returns rate limit error: Pause import, retry with exponential backoff
Ticket conversation incomplete (some messages missing): Log warning, import available messages
Ticket status changed during import: Use snapshot at import start time
User's Teamwork Desk API key revoked: Show "Authentication failed. Please reconnect." error
Import filters return zero tickets: Show "No tickets match your filters" with suggestion to adjust

Error States:

Invalid API credentials: "Could not connect to Teamwork Desk. Please check your domain and API key."
API connection timeout: "Teamwork Desk is not responding. Please try again later."
API returns error (400, 500): Show specific error message from Teamwork Desk
User lacks permission in Teamwork Desk: "Your API key does not have permission to access tickets."
Network error during import: "Import interrupted. Partial data saved. Click 'Retry' to continue."

Out of Scope (Post-MVP):

Two-way sync (updating Teamwork Desk from Foundry)
Real-time webhook integration
Attachment import from tickets
Custom field mapping beyond standard fields
Automatic import scheduling (manual trigger only in MVP)
Multiple Teamwork Desk accounts per project


FEAT-006: Schema Configuration and Mapping
User Stories: US-CFG-001, US-CFG-002
Description:
Visual interface for mapping source data columns to canonical conversation schema. Automatic schema detection with manual override capability.
Functional Requirements:

Automatic schema detection from uploaded files: column names, data types, sample values, null rates
Drag-and-drop interface for mapping source fields to canonical schema fields
Canonical conversation schema fields (MVP): message_id, role (customer/agent), message_text, timestamp, thread_id (optional), metadata (optional JSON)
Field transformation suggestions based on column names (e.g., "email" → "metadata.customer_email")
Mapping validation: Required fields must be mapped, incompatible types flagged
Save mapping configuration per source
Preview transformation output showing first 10 records after mapping

Non-Functional Requirements:

Performance: Schema detection <5 seconds for 100k rows, mapping save <1 second, preview generation <3 seconds
Security: No security-specific requirements (operates on already-uploaded data)
Accessibility: Drag-and-drop has keyboard alternative (select + assign), mapping errors announced to screen readers

Edge Cases:

Source column not mapped: Ignored in output unless required field
Multiple source columns mapped to same target: Use first mapping, warn user of duplicate
Source column name exactly matches canonical field: Auto-map and highlight for user confirmation
User changes mapping after processing: Previous output not affected, re-process required
Source has 100+ columns: Provide search/filter in source column list

Error States:

Required field unmapped: "Field 'role' is required but not mapped. Please map a source column."
Type incompatible (string → number): "Column 'id' contains text but schema expects number. Coercion may fail."
Empty column mapped to required field: "Column 'message' is empty in source. Choose different column."
Circular mapping logic: "Invalid mapping configuration. Check for circular dependencies."

Out of Scope (Post-MVP):

Multiple schema support (Q&A, knowledge base, decision records)
Custom schema definition (MVP uses fixed conversation schema)
Schema versioning
Field-level transformations (concatenation, splitting, regex extraction)
Conditional mapping rules


FEAT-007: PII Detection and De-identification
User Stories: US-CFG-003, US-CFG-004
Description:
Automatic detection of personally identifiable information (PII) in text data with configurable masking strategies to ensure privacy compliance.
Functional Requirements:

Detect PII types: email addresses, phone numbers, person names, postal addresses, credit card numbers (basic validation)
Configure detection sensitivity per PII type: off/low/medium/high
Choose masking strategy per PII type: redact ([REDACTED]), hash (SHA-256), pseudonymize (consistent fake values)
Pseudonymization: Same input always produces same output within project (e.g., "john@example.com" always becomes "user_a1b2c3@example.com")
Preview PII detection results before processing: Show sample text with detected PII highlighted
Detection statistics after processing: Total PII instances found by type, percentage of records with PII
Custom PII patterns (post-MVP): Regex-based detection for organization-specific identifiers

Non-Functional Requirements:

Performance: PII detection processes 1,000 records in <10 seconds, pseudonymization deterministic and fast (SHA-256 based)
Security: Pseudonymization seeds stored encrypted per project, original PII never logged, redacted data irreversible
Accessibility: PII configuration screen fully keyboard-navigable, detection preview screen reader compatible

Edge Cases:

PII detected in metadata fields: Apply same rules as message text
Text contains PII-like strings that aren't PII (e.g., "email support@company.com"): Low sensitivity reduces false positives
Multiple PII instances in single message: All instances masked according to type-specific rules
User disables all PII detection: Warning shown, processing proceeds without masking
Pseudonymization collision (rare): Add random suffix to ensure uniqueness

Error States:

PII detection library fails: "PII detection temporarily unavailable. Processing paused."
Invalid custom regex pattern: "Custom pattern contains syntax error. Please correct."
Pseudonymization key missing: "Configuration error. Please contact support."

Out of Scope (Post-MVP):

Context-aware PII detection (understanding when "apple" is fruit vs. company)
Multi-language PII detection (MVP: English only)
Custom masking templates (e.g., "User [ID]" instead of "[REDACTED]")
PII detection confidence scores
Manual PII annotation interface


FEAT-008: Data Quality Filtering
User Stories: US-CFG-005
Description:
Configurable filtering rules to include only relevant, high-quality records in output datasets based on field values and quality thresholds.
Functional Requirements:

Add filter rules with conditions: field, operator (equals, contains, greater than, less than, regex), value
Combine multiple rules with AND logic (all must match)
Filter by metadata fields (status, priority, date range, tags)
Filter by content quality: message length (min/max characters), contains specific keywords, exclude empty messages
Preview filter results: Show record count before and after applying filters
Filter statistics after processing: Records excluded by each rule, total retention rate

Non-Functional Requirements:

Performance: Filter evaluation adds <5% processing overhead, preview calculation <3 seconds for 100k records
Security: No security-specific requirements
Accessibility: Filter builder keyboard-navigable, operators and values clearly labeled for screen readers

Edge Cases:

Filter excludes all records: Warning before processing, processing allowed but produces empty output
Overlapping filters: No de-duplication needed (AND logic means stricter filtering)
Date filter with invalid format: Show format hint, validate before saving
Filter on field not in source: Warning during configuration, filter skipped during processing
Regex filter with catastrophic backtracking: Timeout after 1 second per record, skip that filter

Error States:

Invalid filter value for type: "Value must be a number for 'priority' field"
Regex syntax error: "Invalid regular expression. Check your pattern."
Filter produces zero results: "No records match your filters. Adjust rules or process without filters."

Out of Scope (Post-MVP):

OR logic between filter rules
Saved filter templates
ML-based quality scoring (toxicity, relevance, coherence)
Filter recommendations based on data distribution
Filter rule explanation/debugging mode


FEAT-009: Dataset Processing Pipeline
User Stories: US-PROC-001, US-PROC-002, US-PROC-003, US-PROC-004
Description:
Batch processing engine that applies configuration (mapping, PII detection, filtering) to source data and produces AI-ready output datasets.
Functional Requirements:

Manual processing trigger: User clicks "Process Dataset" button
Processing stages: ingestion → mapping → PII detection → filtering → output formatting
Progress tracking: Current stage, percentage complete, estimated time remaining
Processing statistics: Records in, records out, PII detected, filter impact
Processing history: List of all processing runs with timestamp, status, configuration snapshot
Failed processing: Detailed error logs, ability to retry
Re-processing: Re-run with updated configuration, replaces previous output

Non-Functional Requirements:

Performance: Process 10,000 records in <2 minutes, 100,000 records in <15 minutes, support up to 1M records per project
Security: Processing runs in isolated queue per organization, output datasets encrypted at rest
Accessibility: Processing status updates announced to screen readers, progress bar semantically labeled

Edge Cases:

Processing started twice simultaneously: Second request queued, not rejected
Processing interrupted by server restart: Job marked "failed", can be restarted
Source data changed during processing: Use snapshot taken at processing start
Configuration changed during processing: Current run uses old config, future runs use new config
Zero records pass filters: Processing completes successfully, output is empty dataset

Error States:

Source data corrupt during processing: "Processing failed due to data corruption. Please re-upload source."
Out of memory during processing: "Dataset too large for available resources. Try filtering data first."
Mapping error (type coercion fails): "Processing failed. Review mapping configuration."
PII detection service unavailable: "Processing paused. PII detection temporarily unavailable. Will retry automatically."

Out of Scope (Post-MVP):

Automatic scheduled processing
Incremental processing (process only new records)
Distributed processing (parallel execution)
Real-time streaming processing
Processing rollback to previous version


FEAT-010: Dataset Export and Download
User Stories: US-EXP-001, US-EXP-002, US-EXP-003, US-EXP-004
Description:
Export processed datasets in multiple AI-ready formats with download management and data lineage documentation.
Functional Requirements:

Export formats: JSONL (newline-delimited JSON), CSV, Q&A Pairs JSON (for RAG systems)
JSONL format: One JSON object per line, compressed with gzip for files >1MB
CSV format: Flattened structure with dot notation for nested fields (e.g., metadata.customer_id)
Q&A Pairs format: Array of {question: "", answer: ""} objects derived from conversation exchanges
Download via browser: Generate pre-signed URL valid for 1 hour
Multiple simultaneous downloads: Up to 3 active downloads per user
Data lineage report: PDF showing source files, transformations, PII masking, filters, output statistics

Non-Functional Requirements:

Performance: Generate export file <30 seconds for 50,000 records, download speed limited only by network
Security: Pre-signed URLs expire after 1 hour, downloads require authentication, export files deleted after 24 hours
Accessibility: Download interface keyboard-accessible, download progress announced to screen readers

Edge Cases:

User closes browser during download: Download continues server-side, can resume via download link
Download link shared with another user: Link validates organization membership, rejects if different org
Export requested while processing: Blocked with "Dataset still processing. Wait for completion."
User requests export multiple times: Each request generates new file (no caching in MVP)
Dataset too large for format (CSV memory limits): Show warning, suggest JSONL format

Error States:

Export generation fails: "Failed to generate export. Please try again or contact support."
Download interrupted: "Download interrupted. Click 'Retry' to resume."
Pre-signed URL expired: "Download link expired. Generate new download."
Format incompatible with data: "Q&A format requires conversational data. Your dataset structure is incompatible."

Out of Scope (Post-MVP):

Push to cloud storage (S3, GCS)
API-based programmatic export
Email delivery of export files
Export scheduling (automatic weekly exports)
Incremental export (only changed records)
Custom export templates


6. MVP Definition
MVP Scope Statement
A web application where non-technical users can upload CSV files or connect Teamwork Desk, configure PII masking and data mapping through a visual interface, process datasets with one click, and download AI-ready outputs in JSONL, CSV, or Q&A formats within 15 minutes from first upload.
MVP Feature List with Removal Tests
Feature IDFeature NameRemoval Test ResultJustification if KeptFEAT-001Authentication and AuthorizationCANNOT removeMulti-tenant SaaS requires user accounts and organization isolationFEAT-002Organization and Team ManagementCANNOT removeTeam collaboration is core value propositionFEAT-003Project ManagementCANNOT removeProjects organize all workflows; without them, data preparation has no contextFEAT-004File Upload Data SourcesCANNOT removeFile upload is primary MVP data source ("aha moment" in <5 min)FEAT-005Teamwork Desk API IntegrationCANNOT removeMandatory API integration per PRD; without it, limited to file uploads onlyFEAT-006Schema Configuration and MappingCANNOT removeSchema normalization is core platform value; without mapping, output isn't AI-readyFEAT-007PII Detection and De-identificationCANNOT removePrivacy compliance is key differentiator; without it, users can't safely use dataFEAT-008Data Quality FilteringCANNOT removeQuality filtering ensures AI systems receive relevant data; without it, output contains noiseFEAT-009Dataset Processing PipelineCANNOT removeProcessing engine is the core transformation layerFEAT-010Dataset Export and DownloadCANNOT removeUsers must retrieve output datasets; without export, platform has no value
MVP Decisions:

Features kept: All 10 features (FEAT-001 through FEAT-010)
Features removed: None - all features pass removal test
Rationale: Each feature is critical to delivering "business data to AI-ready datasets" value proposition

MVP Success Criteria
Launch Success Criteria:

10 beta organizations successfully prepare first dataset within 48 hours of account creation
Average time from signup to first download: <2 hours
Zero critical security vulnerabilities in auth or data handling
Zero data leaks between organizations
90% of file uploads (<50MB) process successfully without errors

Post-Launch Metrics (First 30 Days):

User Activation Rate: 70% (users who complete first processing and download)
Weekly Active Organizations: 50 organizations
Dataset Processing Success Rate: 85% (successful processing / total attempts)
Feature Usage:

File uploads: 80% of projects
Teamwork Desk integration: 40% of projects
PII detection enabled: 90% of projects


Average Processing Time: <5 minutes for 10,000 records


7. Information Architecture
Content Organization
Foundry's information architecture follows a hierarchical model:
Level 1: Application

Global navigation (projects, team, settings)
User context (current organization, user profile)

Level 2: Projects

Project list (dashboard view)
Individual project workspaces

Level 3: Project Components

Data sources (files and API connections)
Configuration (mapping, privacy, filters)
Processing (trigger, monitor, history)
Outputs (exports and lineage)

Level 4: Detail Views

Source data preview
Configuration editors
Processing logs
Export downloads

Navigation Structure
Primary Navigation:

Projects (default landing page after login)
Team (organization members, invitations)
Settings (organization profile, user profile)

User Menu:

Profile Settings
Change Password
Log Out

Breadcrumb Strategy:

Level 1: Organization name (static)
Level 2: Current section (Projects / Team / Settings)
Level 3: Individual item (Project name / Member name)
Level 4: Sub-section (Configuration / Processing / Exports)

Example: Foundry > Projects > Customer Support Agent Training > Configuration
User Flows (Textual Descriptions)
Flow 1: First-Time User Onboarding (Invitation-Based)

User receives invitation email with link
User clicks link, directed to registration page with organization pre-filled
User enters name and password, submits form
System creates account, logs user in automatically
User sees "Welcome" modal explaining Projects concept
User clicks "Create First Project"
Flow continues to Project Creation

Error Branches:

If invitation expired: Show "Invitation expired" message, provide "Request new invite" option
If password too weak: Show validation error inline, keep form filled, focus password field
If email already exists: Show "Account exists, please log in" with link to login page

Flow 2: Create Project and Upload File

User on Projects dashboard, clicks "New Project"
Modal appears with Project Name and Description fields
User enters name (required), description (optional), clicks "Create"
System creates project, user redirected to empty project page
User sees "Add Data Source" empty state, clicks "Upload File"
File picker dialog opens, user selects CSV file
Upload progress bar displays, file uploads to server
System detects schema, shows preview of first 100 rows
User clicks "Confirm", file added to project sources
User sees configuration prompt: "Configure this source to continue"

Error Branches:

If project name empty: Show "Project name required" error, keep modal open
If file >100MB: Show "File too large" error before upload starts
If file type unsupported: Show "Unsupported format" error after selection
If upload interrupted: Show "Upload failed" with "Retry" button

Flow 3: Configure Mapping and Process Dataset

User on project page with uploaded source, clicks "Configure Mapping"
Mapping interface loads showing source columns on left, canonical schema on right
System suggests auto-mappings based on column names (highlighted in blue)
User drags "message" column to "message_text" schema field
User drags "sender_type" column to "role" schema field
User drags "timestamp" column to "timestamp" schema field
User clicks "Save Mapping", configuration saved
User clicks "Next: Privacy Settings"
Privacy settings page shows PII detection toggles (Email, Phone, Names enabled by default)
User selects "Pseudonymize" for emails, "Redact" for phone numbers
User clicks "Save & Process Dataset"
Processing starts, progress bar shows "Mapping... 25%"
Progress updates: "Detecting PII... 60%", "Filtering... 80%", "Generating output... 95%"
Processing completes (2 minutes elapsed), status changes to "Ready"
User sees "Download Dataset" button and processing statistics

Error Branches:

If required field unmapped: Show "Cannot process" error, highlight missing field
If mapping incompatible: Show type mismatch warning, suggest fix
If processing fails: Show detailed error, suggest configuration review
If user navigates away during processing: Processing continues, resumeable from project page

Flow 4: Connect Teamwork Desk and Import Tickets

User on project page, clicks "Add Data Source" → "Connect API"
Connector selection modal shows Teamwork Desk icon
User clicks Teamwork Desk, authentication form appears
User enters Teamwork Desk domain (e.g., "company.teamwork.com")
User enters API key, clicks "Connect"
System validates credentials (3 second check), shows "Connected" status
User clicks "Configure Import"
Filter configuration page shows date range, status, inbox, tags options
User sets date range: "Last 90 days", status: "Closed", inbox: "Support"
User clicks "Preview Import", system shows "~450 tickets will be imported"
User clicks "Start Import", import begins
Progress bar shows "Importing tickets... 120 of 450"
Import completes (3 minutes elapsed), source added to project
User sees source preview with ticket conversations
Flow continues to Configure Mapping

Error Branches:

If credentials invalid: Show "Authentication failed" error, keep form filled
If API connection timeout: Show "Teamwork Desk not responding" error, offer retry
If filters return zero tickets: Show "No tickets match filters" message, suggest adjustment
If import interrupted: Show partial import status, offer "Resume Import" option

Flow 5: Download Processed Dataset

User on project page with "Ready" status, clicks "Download Dataset"
Export format modal appears showing JSONL, CSV, Q&A Pairs options
User selects "JSONL", clicks "Generate Export"
System generates file (10 seconds), shows download link
User clicks "Download" link
Browser downloads file: "project-name-2026-01-21.jsonl.gz"
Download completes, user can open in text editor or AI platform
User returns to project page, export available in "Recent Exports" section

Error Branches:

If export generation fails: Show "Export failed" error, offer "Retry"
If download interrupted: Show "Download incomplete", offer "Resume Download"
If export link expired: Show "Link expired", offer "Generate New Export"

Page/Screen Inventory (MANDATORY)
Page NameRoutePurposeUser StoriesMVP StatusLogin/loginUser authenticationUS-AUTH-002MVPRegister/registerNew user account creationUS-AUTH-001MVPForgot Password/forgot-passwordPassword reset requestUS-AUTH-003MVPReset Password/reset-password/:tokenComplete password resetUS-AUTH-003MVPAccept Invitation/accept-invite/:tokenTeam invitation acceptanceUS-AUTH-004MVPProjects Dashboard/dashboardList all organization projectsUS-PROJ-002MVPProject Detail/projects/:idView project, sources, configuration, processing, outputsUS-PROJ-001, US-SRC-001, US-CFG-002, US-PROC-001, US-EXP-001MVPSource Upload/projects/:id/uploadFile upload interfaceUS-SRC-001, US-SRC-002, US-SRC-003MVPSource Preview/projects/:id/sources/:sourceIdData preview tableUS-SRC-005MVPAPI Connector Setup/projects/:id/connectConnect Teamwork DeskUS-TWD-001, US-TWD-002MVPSchema Mapping/projects/:id/configure/mappingVisual mapping interfaceUS-CFG-002MVPPrivacy Settings/projects/:id/configure/privacyPII detection and masking rulesUS-CFG-003, US-CFG-004MVPFilter Configuration/projects/:id/configure/filtersData quality filtering rulesUS-CFG-005MVPProcessing Status/projects/:id/processingMonitor processing progressUS-PROC-002MVPProcessing History/projects/:id/historyView past processing runsUS-PROC-003MVPExport Downloads/projects/:id/exportsDownload processed datasetsUS-EXP-001, US-EXP-002, US-EXP-003MVPData Lineage/projects/:id/lineageView transformation audit trailUS-EXP-004MVPTeam Management/teamOrganization members and invitationsUS-ORG-002, US-AUTH-004MVPOrganization Settings/settings/organizationOrganization profileUS-ORG-001MVPProfile Settings/settings/profileUser profile and passwordUS-AUTH-003MVP
Page Inventory Rules:

Every user story involving user interaction maps to at least one page ✓
Every page has a defined route ✓
Auth-related pages (login, register, forgot-password, reset-password, accept-invite) are included ✓
Settings pages are included ✓
Admin pages: N/A (no admin-specific features in MVP beyond organization settings)

Usage Downstream:

Agent 5 (UI/UX) uses this to generate route configurations ✓
Agent 6 (Implementation) creates page implementation tasks from this ✓
Agent 7 (QA) verifies all pages exist ✓
Agent 8 (Code Review) audits frontend completeness against this ✓


8. Assumptions and Constraints
Technical Assumptions (Replit Context)
AssumptionRationaleDeployment platform: ReplitTarget platform per agent chain frameworkDatabase: PostgreSQL (Replit-managed via Neon)Replit's standard database offeringArchitecture: Monolithic full-stackSingle container deployment model per Replit constraintsFrontend: React with TypeScriptModern web framework, TypeScript for type safetyBackend: Express.js with TypeScriptNode.js backend framework, consistent languageORM: DrizzleType-safe database ORM per ConstitutionAuthentication: JWT-basedPer Constitution Section C, 24h token expiryFile Storage: Replit filesystem (ephemeral) or external storageReplit filesystem not persistent, requires external storage for uploads >session lifetime
Business Assumptions
AssumptionImpact if WrongMitigationTarget users are comfortable with web-based SaaS toolsLow adoption if users require desktop softwareValidate with 10 user interviews during betaOrganizations willing to trust cloud platform with operational dataPrivacy concerns block adoptionEmphasize encryption, SOC2 compliance, data residency options in messagingTeamwork Desk sufficient API connector for MVP validationLimited market if Teamwork Desk not widely usedAdd 2-3 more connectors (Zendesk, Intercom) in months 2-3Users willing to manually trigger processing (no automatic scheduling)Friction if users expect real-time updatesInstrument usage to measure demand for automatic processingCSV and JSONL formats sufficient for AI vendorsAdoption blocked if vendors require proprietary formatsSurvey 20 AI vendors on required formats before launchPII detection English-only sufficient for MVPLimits international expansionPlan multi-language support for Q2 based on demand signals$49-99/month price point acceptableRevenue model fails if too expensiveValidate pricing with 30 target customers
Known Constraints
ConstraintImplicationSingle container deploymentCannot use microservices architectureReplit resource limitsProcessing limited to 1M records per project in MVPWeb browser access onlyNo mobile app, desktop app, or CLI in MVPManual batch processingNo real-time streaming or automatic scheduling in MVP30-day source data retentionSource files auto-deleted after 30 days to manage storage costsFile size limit: 100MBLarge file uploads require splitting or API connectorProcessing time: Up to 15 minutes for 100k recordsNot suitable for real-time use casesEnglish-only PII detectionNon-English data may have reduced accuracy
Risks and Mitigations
RiskLikelihoodImpactMitigationTeamwork Desk API rate limits cause slow importsHighMediumImplement request queuing with exponential backoff; cache API responses; show estimated time to userPII detection has false positives/negativesMediumHighProvide preview before processing; allow manual override; instrument accuracy to improve detection rulesUsers upload datasets larger than processing capacityMediumHighEnforce 100MB file size limit; show record count estimates; provide guidance on filtering large datasetsSource data schema changes break saved mappingsMediumMediumVersion mappings; detect schema drift on re-upload; prompt user to review mappingProcessing failures due to malformed dataMediumMediumComprehensive input validation; detailed error messages; ability to skip malformed recordsEphemeral Replit filesystem loses uploaded filesLowHighImplement external storage (S3-compatible); upload directly to persistent storageOrganization data leakage between tenantsLowCriticalEnforce organization_id in all database queries; security audit before launch; penetration testing

9. Success Metrics
Key Performance Indicators (KPIs)
KPIMeasurement MethodLaunch Target6-Month TargetUser Activation Rate% users completing first dataset download70%80%Weekly Active OrganizationsUnique organizations using platform per week50300Average Time to First ValueMedian time from signup to first download<2 hours<1 hourDataset Processing Success Rate% processing jobs completing without error85%95%Data Source Diversity% projects using API connectors vs. files only40%60%PII Detection Adoption% projects with PII detection enabled90%95%Re-processing Rate% projects re-running processing monthly30%50%Feature Completion Rate% projects reaching "Ready" status60%75%Processing SpeedMedian processing time per 10k records<5 min<3 minTeam CollaborationAvg team members per organization35
Analytics Requirements
Required Tracking:
User Journey Events:

User Registration: Track invitation source, time to complete registration
Project Creation: Track time to first project, project name patterns
Source Upload: Track file type, file size, upload duration, success/failure rate
API Connection: Track connector type, connection success rate, time to connect
Mapping Configuration: Track auto-mapping acceptance rate, manual mapping time
PII Configuration: Track enabled PII types, masking strategies chosen
Processing Trigger: Track processing start, duration, success/failure, error types
Export Download: Track export format, file size, download completion rate

Product Usage Metrics:

Feature adoption: % users using file upload vs. API vs. both
Configuration complexity: Avg mappings per project, avg filters per project
Data volume: Total records processed per organization, avg dataset size
Processing performance: P50/P95/P99 processing time by record count
Error rates: Processing failures by error type, API failures by connector
Re-engagement: Days since last login, days since last processing run

Business Metrics:

Organization lifecycle: Signup to first value, retention by cohort
Team growth: Invitations sent, invitation acceptance rate, active members
Platform health: Processing queue depth, API availability, storage usage
Support indicators: Error rate trends, configuration abandonment rate


10. Glossary
TermDefinitionOrganizationMulti-tenant entity representing a company or team. All users, projects, and data are isolated within an organization.ProjectWorkspace for preparing a single AI dataset. Contains sources, configuration, processing history, and outputs.SourceOrigin of raw data. Can be uploaded file (CSV, Excel, JSON) or API connector (Teamwork Desk).Canonical SchemaStandardized data structure that sources map to. MVP uses conversation schema with fields: message_id, role, message_text, timestamp, thread_id, metadata.Processing PipelineOrdered stages transforming raw source data into AI-ready output: ingestion → mapping → PII detection → filtering → formatting.PII (Personally Identifiable Information)Data that identifies an individual: emails, phone numbers, names, addresses, account IDs. Must be masked for privacy compliance.De-identificationProcess of removing or masking PII from datasets using redaction, hashing, or pseudonymization.RedactionMasking strategy that replaces PII with placeholder text like [REDACTED]. Irreversible.PseudonymizationMasking strategy that replaces PII with consistent fake values (e.g., same email always becomes same pseudonym). Maintains referential integrity.MappingConfiguration linking source data columns to canonical schema fields (e.g., "email_body" → "message_text").Filter RulesConditions determining which records are included in output (e.g., status = "closed", created_after = "2024-01-01").AI-Ready DatasetProcessed output in format suitable for AI training, fine-tuning, or RAG systems. Clean, de-identified, structured.JSONL (JSON Lines)File format with one JSON object per line. Standard for AI training data. Files often gzip-compressed.Q&A PairsDataset format for retrieval-augmented generation (RAG) systems. Each entry has question and answer fields.Data LineageAudit trail showing source data, transformations applied, configuration used, and output generated. Required for compliance.MVP (Minimum Viable Product)Initial product version with essential features to deliver core value. Foundry MVP: file upload + Teamwork Desk + PII detection + export.Teamwork DeskCustomer support helpdesk platform with API for accessing tickets and conversations.API ConnectorIntegration allowing Foundry to automatically import data from external platforms (Teamwork Desk, future: Zendesk, Salesforce).Role-Based Access Control (RBAC)Permission system with two roles: admin (full access) and member (view/use projects, no team management).JWT (JSON Web Token)Authentication token standard. Foundry uses JWT with 24h expiry per Constitution.ConstitutionFramework document defining non-negotiable technical standards (API conventions, auth patterns, Replit requirements).

VALIDATION FOOTER (MANDATORY)
Completeness Check

 All 10 sections populated with substantial content
 All personas have 3+ user stories (Sarah: 12, Marcus: 10, Jennifer: 6, David: 3)
 All user stories have 2+ acceptance criteria
 All MVP features have documented removal test
 All features trace to user stories
 All user stories trace to personas
 All user flows include error states
 Page inventory complete with all pages listed (18 pages total)
 Technical assumptions compatible with Replit

Traceability Matrix
User Story IDFeature IDPage(s)StatusUS-AUTH-001FEAT-001/accept-invite/:token✓US-AUTH-002FEAT-001/login, /dashboard✓US-AUTH-003FEAT-001/forgot-password, /reset-password/:token✓US-AUTH-004FEAT-001, FEAT-002/team, /accept-invite/:token✓US-ORG-001FEAT-002/settings/organization✓US-ORG-002FEAT-002/team✓US-PROJ-001FEAT-003/dashboard, /projects/:id✓US-PROJ-002FEAT-003/dashboard✓US-PROJ-003FEAT-003/projects/:id✓US-PROJ-004FEAT-003/projects/:id✓US-SRC-001FEAT-004/projects/:id/upload, /projects/:id/sources/:sourceId✓US-SRC-002FEAT-004/projects/:id/upload✓US-SRC-003FEAT-004/projects/:id/upload✓US-SRC-004FEAT-005/projects/:id/connect✓US-SRC-005FEAT-004/projects/:id/sources/:sourceId✓US-CFG-001FEAT-006/projects/:id/configure/mapping✓US-CFG-002FEAT-006/projects/:id/configure/mapping✓US-CFG-003FEAT-007/projects/:id/configure/privacy✓US-CFG-004FEAT-007/projects/:id/configure/privacy✓US-CFG-005FEAT-008/projects/:id/configure/filters✓US-PROC-001FEAT-009/projects/:id/processing✓US-PROC-002FEAT-009/projects/:id/processing✓US-PROC-003FEAT-009/projects/:id/history✓US-PROC-004FEAT-009/projects/:id/processing✓US-EXP-001FEAT-010/projects/:id/exports✓US-EXP-002FEAT-010/projects/:id/exports✓US-EXP-003FEAT-010/projects/:id/exports✓US-EXP-004FEAT-010/projects/:id/lineage✓US-TWD-001FEAT-005/projects/:id/connect✓US-TWD-002FEAT-005/projects/:id/connect✓US-TWD-003FEAT-005/projects/:id/connect✓US-TWD-004FEAT-005/projects/:id/configure/mapping✓US-TWD-005FEAT-005/projects/:id/sources/:sourceId✓
Prompt Maintenance Contract
If this prompt is edited, you MUST:

Update the version history with changes and Hygiene Gate: PASS
Re-run all Prompt Hygiene Gate checks (per Constitution Section L)
Confirm encoding is clean (no mojibake or non-ASCII artifacts)
Verify no global rules are restated (reference Constitution instead)

If any check fails, the prompt update is invalid and must not be delivered.
Prompt Hygiene Gate (per Constitution Section L)

 Framework Version header present and correct
 Encoding scan passed: No non-ASCII artifact tokens
 Inheritance statement references Constitution v3.3
 No full restatement of global rules (uses "Per Constitution Section X" references)

Confidence Scores
SectionScore (1-10)NotesProblem Statement9Clear quantified pain from Foundry briefPersonas9Well-defined with specific pain pointsUser Stories8Comprehensive coverage, some technical complexity in configuration storiesMVP Scope10All features pass removal test, clear rationaleReplit Compatibility10All assumptions align with Replit constraintsOverall9Strong PRD ready for downstream agents
Flagged Items Requiring Review

File Storage Strategy: Replit filesystem is ephemeral. For production, files must be stored in external storage (S3-compatible). Implementation Plan should specify external storage integration as SETUP task.
Processing Scale Limits: PRD assumes 1M records max per project on Replit. If beta testing shows demand for larger datasets, may need dedicated processing infrastructure.
PII Detection Accuracy: English-only detection in MVP. Multi-language support required for international expansion. Should instrument false positive/negative rates in beta.

Document Status: COMPLETE

DOWNSTREAM AGENT HANDOFF BRIEF
Deployment Context (All Agents)
Per Constitution Section C and Section D: All global platform conventions and Replit non-negotiables apply.
This context applies to all downstream agents. Do not specify infrastructure that conflicts with the Constitution.
For Agent 2: System Architecture
Core Technical Challenges:

File upload and storage: Need persistent storage strategy (Replit filesystem ephemeral)
Large dataset processing: 100k+ records in <15 minutes requires efficient processing architecture
OAuth integration: Teamwork Desk API requires encrypted token storage per Constitution

Scale Expectations:

Concurrent users: 50-200 organizations in first 6 months (within Replit limits)
Data volume: 1M records max per project, 10GB storage per organization
API throughput: 50 requests/min average, 200 requests/min peak

Integration Requirements:

Teamwork Desk API: REST API, OAuth credentials, 120 requests/minute rate limit
External storage (S3-compatible): Required for persistent file storage
Email service (optional): Transactional emails for invitations, password resets

Authentication/Authorization Complexity:

JWT-based auth (24h expiry per Constitution)
Multi-tenant organization isolation (all queries scoped by organization_id)
Role-based access: admin (full permissions) vs. member (limited permissions)

Security Considerations:

OAuth token encryption: AES-256-GCM per Constitution Section C
PII masking: Must happen server-side during processing
Organization data isolation: Critical security boundary
File upload virus scanning: ClamAV or similar

Key Decisions Deferred to You:

External storage provider selection (S3, GCS, Azure Blob)
Processing queue architecture (simple queue vs. job runner)
PII detection library selection (spaCy, presidio, custom)
Email service provider (Resend, SendGrid, AWS SES)

For Agent 3: Data Modeling
Primary Entities Implied:

User (from US-AUTH-001, US-AUTH-002, US-ORG-002)
Organization (from US-ORG-001, multi-tenancy requirement)
Invitation (from US-AUTH-004, 7-day expiry)
Project (from US-PROJ-001, US-PROJ-002)
Source (from US-SRC-001, US-TWD-001 - polymorphic: file or API)
SourceFile (file metadata, storage location)
APIConnection (Teamwork Desk credentials, configuration)
SchemaMapping (from US-CFG-002, maps source fields to canonical schema)
PIIConfiguration (from US-CFG-003, US-CFG-004, detection rules)
FilterRule (from US-CFG-005, data filtering conditions)
ProcessingJob (from US-PROC-001, status tracking)
ProcessingStatistics (from US-PROC-003, metrics)
Export (from US-EXP-001, download links, expiry)

Key Relationships:

User -> Organization: Many-to-one (user belongs to one org)
Organization -> User: One-to-many (org has many users)
Organization -> Project: One-to-many
Project -> Source: One-to-many (project can have multiple sources)
Source -> SourceFile: One-to-one (polymorphic)
Source -> APIConnection: One-to-one (polymorphic)
Project -> SchemaMapping: One-to-one (one mapping config per project)
Project -> PIIConfiguration: One-to-one
Project -> FilterRule: One-to-many (multiple filter rules)
Project -> ProcessingJob: One-to-many (processing history)
ProcessingJob -> ProcessingStatistics: One-to-one
ProcessingJob -> Export: One-to-many (multiple export formats)

Data Lifecycle Considerations:

Retention: Source files auto-delete after 30 days
Deletion: Soft delete projects (preserve audit trail)
Archival: Processed datasets deleted when project deleted

Multi-Tenancy Requirements:

All tables have organization_id foreign key
All queries scoped by organization_id (enforce with Row-Level Security or application layer)
Invitation tokens link to organization but not user (user created on acceptance)

For Agent 4: API Contract
Primary Operations Needed:
Auth:

POST /api/auth/register (with optional invitation token)
POST /api/auth/login
POST /api/auth/logout
POST /api/auth/forgot-password
POST /api/auth/reset-password
GET /api/auth/me
PATCH /api/auth/profile

Organizations:

GET /api/organizations/current
PATCH /api/organizations/current

Team:

GET /api/organizations/members
POST /api/organizations/invitations
DELETE /api/organizations/members/:id
PATCH /api/organizations/members/:id (role change)

Projects:

GET /api/projects (list)
POST /api/projects (create)
GET /api/projects/:id (detail)
PATCH /api/projects/:id (update)
DELETE /api/projects/:id (soft delete, post-MVP)

Sources:

POST /api/projects/:projectId/sources/upload (file upload with multipart/form-data)
POST /api/projects/:projectId/sources/api-connect (API connector setup)
GET /api/projects/:projectId/sources (list)
GET /api/projects/:projectId/sources/:sourceId (detail and preview)
DELETE /api/projects/:projectId/sources/:sourceId

Configuration:

GET /api/projects/:projectId/configuration (get all config)
PATCH /api/projects/:projectId/configuration/mapping (schema mapping)
PATCH /api/projects/:projectId/configuration/privacy (PII rules)
POST /api/projects/:projectId/configuration/filters (add filter rule)
DELETE /api/projects/:projectId/configuration/filters/:filterId

Processing:

POST /api/projects/:projectId/processing/start (trigger processing)
GET /api/projects/:projectId/processing/status (current status)
GET /api/projects/:projectId/processing/history (past runs)
GET /api/projects/:projectId/processing/:jobId (job detail)

Exports:

POST /api/projects/:projectId/exports (generate export with format parameter)
GET /api/projects/:projectId/exports (list available exports)
GET /api/projects/:projectId/exports/:exportId/download (pre-signed URL)
GET /api/projects/:projectId/lineage (data lineage report)

Authentication Requirements:

Method: JWT Bearer tokens (per Constitution Section C)
Token expiry: 24h (per Constitution Section C)
Refresh: Not required for MVP (re-login after 24h)

External Integrations:

Teamwork Desk API: OAuth credential validation, ticket import
External Storage API: S3-compatible for file persistence
Email Service API: Transactional emails (invitations, password reset)

Real-Time Requirements:

Processing status updates: Polling (GET every 2 seconds) in MVP, no WebSockets

For Agent 5: UI/UX Specification
Primary User Flows:

Invitation acceptance → Registration → First project creation → File upload → Download (aha moment)
Login → Create project → Connect Teamwork Desk → Configure mapping → Process → Download
Login → Select existing project → Re-configure → Re-process → Download updated dataset
Admin: Invite team member → Member accepts → Collaborates on project

Key Interaction Patterns:

File upload: Drag-and-drop zone with file picker fallback, progress bar
Schema mapping: Visual drag-and-drop interface (source columns → schema fields)
Processing monitor: Real-time progress bar with stage indication (mapping, PII detection, filtering)
Data preview: Infinite-scroll table with fixed header
Configuration wizards: Multi-step forms with "Save & Continue" pattern

Accessibility Requirements:

Target: WCAG 2.1 AA
Specific needs:

Drag-and-drop mapping must have keyboard alternative (select + assign button)
Processing progress announced to screen readers
All form validation errors semantically linked to fields
File upload accessible via keyboard (focus dropzone, press Enter to trigger picker)



Mobile/Responsive Requirements:

Desktop-first (complex data configuration not optimized for mobile in MVP)
Responsive breakpoints: tablets supported, mobile phones show "Desktop experience recommended" message
Touch targets: 44x44px minimum for buttons and interactive elements

For Agent 6: Implementation Orchestrator
Per Constitution Section C/D: Global platform and API conventions apply.
Security Middleware Required:

helmet (security headers)
cors (Replit-compatible origin configuration)
express-rate-limit (rate limiting per Architecture)

Critical Configuration:

Port 5000 (Replit)
Trust proxy (for rate limiting and IP detection)
Vite watch ignored (Replit directories: node_modules, .git, dist)

File Upload Specifics:

Use multer middleware for multipart/form-data
Temporary storage: /tmp directory (ephemeral)
Final storage: S3-compatible external storage
Virus scanning: Integrate before moving to final storage

Processing Architecture:

Simple queue in MVP (no distributed job runner)
Process synchronously with progress updates stored in database
Timeout: 30 minutes per processing job
Cancellation: Graceful shutdown on job stop request

External Dependencies:

AWS SDK (or compatible): For S3 file storage (REQUIRED)
Teamwork Desk API client library: (REQUIRED, evaluate: axios or node-fetch with custom wrapper)
PII detection library: (REQUIRED, options: spaCy.js, presidio, custom regex + NER)
Email service SDK: (OPTIONAL, options: Resend, SendGrid, AWS SES)
Virus scanning: (REQUIRED, ClamAV or cloud service like VirusTotal)

Handoff Summary
MetricValueTotal user stories38MVP stories35Post-MVP stories3User personas4MVP features10Total pages18Estimated complexityS: 8, M: 18, L: 10, XL: 2
Recommended Human Review Points:

External storage provider selection (cost and Replit compatibility)
PII detection library evaluation (accuracy vs. performance trade-off)
Processing timeout strategy (30 min limit vs. partial results)
Email service provider (cost and deliverability)
Virus scanning implementation (local ClamAV vs. cloud API)


ASSUMPTION REGISTER (MANDATORY OUTPUT SECTION)
AR-001: External File Storage Required

Type: DEPENDENCY
Source Gap: Foundry brief doesn't specify how uploaded files are persisted beyond Replit's ephemeral filesystem
Assumption Made: Production requires external S3-compatible storage for uploaded files (retained 30 days)
Impact if Wrong: Files lost on container restart, users cannot re-process datasets, MVP unusable
Proposed Resolution: Architecture agent must specify external storage provider (S3, GCS, Azure Blob) and integration pattern
Status: UNRESOLVED
Owner: Agent 2 (Architecture)
Date: 2026-01-21

AR-002: PII Detection Library Selection

Type: DEPENDENCY
Source Gap: Foundry brief mentions "PII detection" but doesn't specify detection method or library
Assumption Made: Server-side NER (Named Entity Recognition) library required, options: spaCy.js, presidio, custom regex
Impact if Wrong: Inaccurate PII detection causes privacy violations or excessive false positives breaking datasets
Proposed Resolution: Implementation agent must evaluate libraries for accuracy/performance trade-off, recommend best fit
Status: UNRESOLVED
Owner: Agent 6 (Implementation)
Date: 2026-01-21

AR-003: Email Service for Transactional Emails

Type: DEPENDENCY
Source Gap: Foundry brief mentions "invitation emails" and "password reset" but doesn't specify email service
Assumption Made: Transactional email service required (Resend, SendGrid, AWS SES), NOT self-hosted SMTP
Impact if Wrong: Emails not delivered, users cannot reset passwords or accept invitations, onboarding broken
Proposed Resolution: Architecture agent must specify email service provider with fallback graceful degradation (log to console in dev)
Status: UNRESOLVED
Owner: Agent 2 (Architecture)
Date: 2026-01-21

AR-004: Single Organization per User

Type: ASSUMPTION
Source Gap: Foundry brief doesn't explicitly state whether users can belong to multiple organizations
Assumption Made: MVP: Users belong to exactly one organization (standard B2B SaaS pattern)
Impact if Wrong: Need junction table (users_organizations), auth context must track "current organization", UI needs org switcher
Proposed Resolution: Confirm with stakeholder: "Can a single user account belong to multiple organizations?" If yes, update data model
Status: UNRESOLVED
Owner: Human (Product stakeholder)
Date: 2026-01-21

AR-005: Teamwork Desk API Rate Limits

Type: RISK
Source Gap: Foundry brief mentions Teamwork Desk but doesn't specify API rate limits or import behavior
Assumption Made: Teamwork Desk API allows 120 requests/minute, imports handled with exponential backoff on rate limit errors
Impact if Wrong: Imports fail or take excessively long if rate limits stricter than assumed
Proposed Resolution: Verify Teamwork Desk API documentation during implementation, implement robust rate limit handling
Status: UNRESOLVED
Owner: Agent 6 (Implementation)
Date: 2026-01-21

AR-006: Processing Timeout Strategy

Type: ASSUMPTION
Source Gap: Foundry brief states "up to 15 minutes for 100k records" but doesn't specify timeout handling for larger datasets
Assumption Made: Processing timeout set to 30 minutes, jobs exceeding timeout marked "failed" with partial results option
Impact if Wrong: Users frustrated by timeout failures, or system resources exhausted by runaway jobs
Proposed Resolution: Instrument processing times during beta, adjust timeout based on actual performance
Status: UNRESOLVED
Owner: Agent 6 (Implementation)
Date: 2026-01-21

AR-007: Conversation Schema is MVP-Only Schema

Type: ASSUMPTION
Source Gap: Foundry brief mentions "canonical schemas" plural but only describes conversation use case
Assumption Made: MVP implements single canonical schema (conversation: message_id, role, message_text, timestamp, thread_id, metadata)
Impact if Wrong: Users wanting knowledge base or decision record schemas blocked, limited use cases
Proposed Resolution: Document post-MVP schema expansion path (Q&A, knowledge base, decision records) in roadmap
Status: ACCEPTED
Owner: Human (Product roadmap)
Date: 2026-01-21

AR-008: 24-Hour Token Expiry Sufficient for MVP

Type: ASSUMPTION
Source Gap: Constitution defines 24h JWT expiry but doesn't consider long-running processing jobs
Assumption Made: 24h token expiry acceptable because processing completes in <30 minutes, user remains logged in
Impact if Wrong: Users logged out during long data preparation sessions, frustrated by re-login interruptions
Proposed Resolution: Monitor session expiry feedback during beta, consider refresh token pattern if needed
Status: UNRESOLVED
Owner: Agent 2 (Architecture)
Date: 2026-01-21