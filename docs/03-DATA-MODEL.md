# Data Model Document: Foundry
**Generated by:** Agent 3 - Data Modeling v18 (Application-Agnostic)  
**Date:** 2026-01-21  
**Status:** Complete  
**Framework Version:** Agent Specification Framework v2.1  
**Constitution:** Agent 0 - Agent Constitution v3.3

---

## Document Purpose

This document translates Foundry's product requirements and architectural decisions into a production-ready PostgreSQL database schema using Drizzle ORM. All schema decisions trace to PRD entities and architectural constraints.

---

## PHASE 1-2: ENTITY ANALYSIS & CHECKPOINT 1

### Entity Extraction from PRD

**Core Business Entities:**
- **users** - System authentication and user profiles
- **organisations** - Multi-tenant isolation containers
- **projects** - User-created data preparation workspaces
- **data_sources** - File uploads and API connections
- **schema_mappings** - Column mapping configurations (source → canonical)
- **processing_jobs** - Async data transformation jobs
- **datasets** - Processed output artifacts
- **oauth_connections** - Encrypted OAuth tokens for Teamwork Desk
- **team_members** - Organisation collaboration with roles
- **audit_logs** - Compliance and activity tracking

### Key Relationships Identified

```
organisations 1───< users (organisationId)
organisations 1───< projects (organisationId)
organisations 1───< oauth_connections (organisationId)

users 1───< projects (userId - creator)
projects 1───< data_sources (projectId)
projects 1───< schema_mappings (projectId)
projects 1───< processing_jobs (projectId)
projects 1───< datasets (projectId)

data_sources 1───< processing_jobs (dataSourceId)
processing_jobs 1───< datasets (jobId)

organisations 1───< team_members >───1 users (many-to-many via junction)
```

### Multi-Tenancy Strategy

Row-level isolation via `organisationId` foreign key on all tenant-scoped tables. Global tables: `users`, `organisations`. Tenant-scoped tables: `projects`, `data_sources`, `schema_mappings`, `processing_jobs`, `datasets`, `oauth_connections`, `team_members`, `audit_logs`.

### Assumptions Documented

**AR-DATA-001:** Single organisation per user (MVP)
- Users belong to exactly one organisation via `organisationId` column
- If multi-org support needed post-MVP, requires junction table `users_organisations`

**AR-DATA-002:** Soft delete for user-generated content
- `projects`, `data_sources`, `datasets` use `deletedAt` timestamp pattern
- Enables recovery and audit trail per PRD compliance requirements

**AR-DATA-003:** Processing jobs state machine
- States: pending, processing, completed, failed, cancelled
- No partial completion state in MVP (per Architecture timeout strategy)

**Continuing immediately to entity design...**

---

## PHASE 3-4: ENTITY DESIGN & RELATIONSHIPS

### 1. Core Authentication & Multi-Tenancy

#### organisations

**Purpose:** Multi-tenant isolation containers

```typescript
export const organisations = pgTable('organisations', {
  id: serial('id').primaryKey(),
  name: text('name').notNull(),
  slug: text('slug').notNull().unique(), // URL-safe identifier: acme-corp
  subscriptionTier: text('subscription_tier').notNull().default('free'), // free, pro, enterprise
  subscriptionStatus: text('subscription_status').notNull().default('active'), // active, suspended, cancelled
  createdAt: timestamp('created_at').defaultNow().notNull(),
  updatedAt: timestamp('updated_at').defaultNow().notNull(),
});

export type Organisation = typeof organisations.$inferSelect;
export type NewOrganisation = typeof organisations.$inferInsert;
```

**Columns:**
- `id` - Primary key
- `name` - Display name (e.g., "Acme Corporation")
- `slug` - URL-safe unique identifier for routing/subdomains
- `subscriptionTier` - Billing tier (feature gating)
- `subscriptionStatus` - Operational status
- `createdAt`, `updatedAt` - Audit columns (standard)

**Constraints:**
- `slug` UNIQUE - prevents duplicate organisation identifiers
- NOT NULL on `name`, `subscriptionTier`, `subscriptionStatus`

---

#### users

**Purpose:** User authentication and profiles

```typescript
export const users = pgTable('users', {
  id: serial('id').primaryKey(),
  organisationId: integer('organisation_id').notNull().references(() => organisations.id, { onDelete: 'restrict' }),
  email: text('email').notNull().unique(),
  passwordHash: text('password_hash').notNull(),
  name: text('name'),
  role: text('role').notNull().default('user'), // user, admin
  status: text('status').notNull().default('active'), // active, suspended, invited
  invitedBy: integer('invited_by').references(() => users.id, { onDelete: 'set null' }),
  lastLoginAt: timestamp('last_login_at'),
  createdAt: timestamp('created_at').defaultNow().notNull(),
  updatedAt: timestamp('updated_at').defaultNow().notNull(),
});

export type User = typeof users.$inferSelect;
export type NewUser = typeof users.$inferInsert;
```

**Columns:**
- `id` - Primary key
- `organisationId` - Organisation membership (FK to organisations)
- `email` - Login credential, globally unique
- `passwordHash` - bcrypt hashed password
- `name` - Optional display name
- `role` - System-level role (user, admin)
- `status` - Account state (active, suspended, invited)
- `invitedBy` - Referral tracking for team invitations
- `lastLoginAt` - Security audit timestamp
- `createdAt`, `updatedAt` - Audit columns

**Constraints:**
- `email` UNIQUE - single account per email
- `organisationId` NOT NULL - every user belongs to one organisation
- FK `organisationId` RESTRICT - cannot delete org with users
- FK `invitedBy` SET NULL - preserve user if inviter deleted

**Indexes:**
- `email` - automatic via UNIQUE constraint
- `organisationId` - required for multi-tenant queries

---

#### team_members

**Purpose:** Organisation role-based access control

```typescript
export const teamMembers = pgTable('team_members', {
  id: serial('id').primaryKey(),
  organisationId: integer('organisation_id').notNull().references(() => organisations.id, { onDelete: 'cascade' }),
  userId: integer('user_id').notNull().references(() => users.id, { onDelete: 'cascade' }),
  role: text('role').notNull(), // admin, editor, viewer
  createdAt: timestamp('created_at').defaultNow().notNull(),
  updatedAt: timestamp('updated_at').defaultNow().notNull(),
}, (table) => ({
  // Unique constraint: user can have only one role per organisation
  orgUserUnique: unique('org_user_unique').on(table.organisationId, table.userId),
}));

export type TeamMember = typeof teamMembers.$inferSelect;
export type NewTeamMember = typeof teamMembers.$inferInsert;
```

**Columns:**
- `id` - Primary key
- `organisationId` - Scoped to organisation
- `userId` - Team member user account
- `role` - Organisation role (admin, editor, viewer)
- `createdAt`, `updatedAt` - Audit columns

**Constraints:**
- UNIQUE (`organisationId`, `userId`) - one role per user per org
- FK CASCADE - delete membership when user or org deleted

**Indexes:**
- Composite unique index on (`organisationId`, `userId`) - automatic via UNIQUE constraint
- `userId` - for user's organisation queries

---

### 2. Data Preparation Workflow

#### projects

**Purpose:** User-created data preparation workspaces

```typescript
export const projects = pgTable('projects', {
  id: serial('id').primaryKey(),
  organisationId: integer('organisation_id').notNull().references(() => organisations.id, { onDelete: 'cascade' }),
  userId: integer('user_id').notNull().references(() => users.id, { onDelete: 'restrict' }),
  name: text('name').notNull(),
  description: text('description'),
  canonicalSchema: text('canonical_schema').notNull().default('conversation'), // conversation (MVP only)
  deletedAt: timestamp('deleted_at'), // Soft delete
  createdAt: timestamp('created_at').defaultNow().notNull(),
  updatedAt: timestamp('updated_at').defaultNow().notNull(),
});

export type Project = typeof projects.$inferSelect;
export type NewProject = typeof projects.$inferInsert;
```

**Columns:**
- `id` - Primary key
- `organisationId` - Multi-tenant isolation
- `userId` - Project creator/owner
- `name` - User-defined project name
- `description` - Optional project notes
- `canonicalSchema` - Target schema type (conversation in MVP)
- `deletedAt` - Soft delete timestamp (null = active)
- `createdAt`, `updatedAt` - Audit columns

**Constraints:**
- FK `organisationId` CASCADE - delete projects when org deleted
- FK `userId` RESTRICT - preserve projects if user deleted (reassign ownership)
- NOT NULL on `name`, `canonicalSchema`

**Soft Delete Pattern:**
- `deletedAt = null` → active project
- `deletedAt = timestamp` → soft deleted, can be restored

**Indexes:**
- `organisationId` - for tenant queries
- `userId` - for "my projects" queries
- Composite (`organisationId`, `deletedAt`) - for active project listings

---

#### data_sources

**Purpose:** File uploads and API connections

```typescript
export const dataSources = pgTable('data_sources', {
  id: serial('id').primaryKey(),
  organisationId: integer('organisation_id').notNull().references(() => organisations.id, { onDelete: 'cascade' }),
  projectId: integer('project_id').notNull().references(() => projects.id, { onDelete: 'cascade' }),
  name: text('name').notNull(),
  type: text('type').notNull(), // file, teamwork_desk
  fileFormat: text('file_format'), // csv, json, xml, xlsx (null for API sources)
  s3Key: text('s3_key'), // S3 object key (null for API sources)
  fileSize: integer('file_size'), // bytes (null for API sources)
  recordCount: integer('record_count'), // estimated/actual record count
  metadata: text('metadata'), // JSON string: original filename, API config, etc.
  deletedAt: timestamp('deleted_at'), // Soft delete
  createdAt: timestamp('created_at').defaultNow().notNull(),
  updatedAt: timestamp('updated_at').defaultNow().notNull(),
});

export type DataSource = typeof dataSources.$inferSelect;
export type NewDataSource = typeof dataSources.$inferInsert;
```

**Columns:**
- `id` - Primary key
- `organisationId` - Multi-tenant isolation
- `projectId` - Belongs to project
- `name` - User-defined source name
- `type` - Source type discriminator (file, teamwork_desk)
- `fileFormat` - File type for uploads (csv, json, xml, xlsx)
- `s3Key` - S3 object identifier for uploaded files
- `fileSize` - File size in bytes (for UI display)
- `recordCount` - Estimated record count (for progress estimation)
- `metadata` - JSON string for flexible attributes (original filename, upload date, API params)
- `deletedAt` - Soft delete timestamp
- `createdAt`, `updatedAt` - Audit columns

**Constraints:**
- FK CASCADE on `organisationId`, `projectId` - delete sources with parent
- NOT NULL on `name`, `type`

**Indexes:**
- `projectId` - for project's sources query
- `organisationId` - for tenant queries
- `type` - for source type filtering

---

#### schema_mappings

**Purpose:** Column mapping configurations (source → canonical schema)

```typescript
export const schemaMappings = pgTable('schema_mappings', {
  id: serial('id').primaryKey(),
  organisationId: integer('organisation_id').notNull().references(() => organisations.id, { onDelete: 'cascade' }),
  projectId: integer('project_id').notNull().references(() => projects.id, { onDelete: 'cascade' }),
  dataSourceId: integer('data_source_id').notNull().references(() => dataSources.id, { onDelete: 'cascade' }),
  mappingConfig: text('mapping_config').notNull(), // JSON: { "message_id": "id", "role": "sender_type", ... }
  validatedAt: timestamp('validated_at'), // null = not validated, timestamp = mapping tested
  createdAt: timestamp('created_at').defaultNow().notNull(),
  updatedAt: timestamp('updated_at').defaultNow().notNull(),
}, (table) => ({
  // One mapping per data source
  dataSourceUnique: unique('data_source_unique').on(table.dataSourceId),
}));

export type SchemaMapping = typeof schemaMappings.$inferSelect;
export type NewSchemaMapping = typeof schemaMappings.$inferInsert;
```

**Columns:**
- `id` - Primary key
- `organisationId` - Multi-tenant isolation
- `projectId` - Belongs to project
- `dataSourceId` - Mapping applies to specific source
- `mappingConfig` - JSON string with source-to-canonical field mappings
- `validatedAt` - Timestamp when mapping successfully tested
- `createdAt`, `updatedAt` - Audit columns

**Mapping Config JSON Structure:**
```json
{
  "message_id": "source_id_column",
  "role": "sender_type_column",
  "message_text": "body_column",
  "timestamp": "created_date_column",
  "thread_id": "conversation_id_column"
}
```

**Constraints:**
- UNIQUE `dataSourceId` - one mapping per source
- FK CASCADE - delete mapping with parent entities
- NOT NULL on `mappingConfig`

**Indexes:**
- `dataSourceId` - automatic via UNIQUE constraint
- `projectId` - for project's mappings query

---

#### processing_jobs

**Purpose:** Async data transformation jobs with progress tracking

```typescript
export const processingJobs = pgTable('processing_jobs', {
  id: serial('id').primaryKey(),
  organisationId: integer('organisation_id').notNull().references(() => organisations.id, { onDelete: 'cascade' }),
  projectId: integer('project_id').notNull().references(() => projects.id, { onDelete: 'cascade' }),
  dataSourceId: integer('data_source_id').notNull().references(() => dataSources.id, { onDelete: 'cascade' }),
  status: text('status').notNull().default('pending'), // pending, processing, completed, failed, cancelled
  progress: integer('progress').notNull().default(0), // 0-100 percentage
  recordsProcessed: integer('records_processed').notNull().default(0),
  recordsTotal: integer('records_total').notNull().default(0),
  piiDetected: integer('pii_detected').notNull().default(0), // Count of PII instances found
  errorMessage: text('error_message'), // Detailed error for failed jobs
  startedAt: timestamp('started_at'),
  completedAt: timestamp('completed_at'),
  createdAt: timestamp('created_at').defaultNow().notNull(),
  updatedAt: timestamp('updated_at').defaultNow().notNull(),
}, (table) => ({
  // Index for polling pending jobs
  statusIdx: index('processing_jobs_status_idx').on(table.status),
  // Index for organisation's active jobs
  orgStatusIdx: index('processing_jobs_org_status_idx').on(table.organisationId, table.status),
}));

export type ProcessingJob = typeof processingJobs.$inferSelect;
export type NewProcessingJob = typeof processingJobs.$inferInsert;
```

**Columns:**
- `id` - Primary key
- `organisationId` - Multi-tenant isolation
- `projectId` - Job belongs to project
- `dataSourceId` - Processing specific source
- `status` - State machine: pending → processing → completed/failed/cancelled
- `progress` - UI progress indicator (0-100)
- `recordsProcessed` - Current count (for progress calculation)
- `recordsTotal` - Total records to process
- `piiDetected` - Privacy audit metric
- `errorMessage` - Failure diagnostics
- `startedAt` - Job start timestamp
- `completedAt` - Job finish timestamp
- `createdAt`, `updatedAt` - Audit columns

**State Machine:**
```
pending → processing → completed
                    → failed
                    → cancelled
```

**Constraints:**
- FK CASCADE - delete job with parent entities
- NOT NULL on `status`, `progress`, `recordsProcessed`, `recordsTotal`, `piiDetected`
- CHECK `progress` BETWEEN 0 AND 100 (enforced in application layer)

**Indexes:**
- `status` - for worker polling pending jobs
- Composite (`organisationId`, `status`) - for UI active jobs list
- `projectId` - for project job history

---

#### datasets

**Purpose:** Processed output artifacts

```typescript
export const datasets = pgTable('datasets', {
  id: serial('id').primaryKey(),
  organisationId: integer('organisation_id').notNull().references(() => organisations.id, { onDelete: 'cascade' }),
  projectId: integer('project_id').notNull().references(() => projects.id, { onDelete: 'cascade' }),
  jobId: integer('job_id').notNull().references(() => processingJobs.id, { onDelete: 'cascade' }),
  name: text('name').notNull(),
  description: text('description'),
  format: text('format').notNull(), // csv, json, jsonl, parquet
  s3Key: text('s3_key').notNull(), // S3 object key for export file
  fileSize: integer('file_size').notNull(), // bytes
  recordCount: integer('record_count').notNull(),
  piiRedacted: integer('pii_redacted').notNull().default(0), // Count of redacted PII instances
  checksumSha256: text('checksum_sha256').notNull(), // File integrity verification
  expiresAt: timestamp('expires_at'), // Optional expiration (30 days per Architecture)
  deletedAt: timestamp('deleted_at'), // Soft delete
  createdAt: timestamp('created_at').defaultNow().notNull(),
  updatedAt: timestamp('updated_at').defaultNow().notNull(),
});

export type Dataset = typeof datasets.$inferSelect;
export type NewDataset = typeof datasets.$inferInsert;
```

**Columns:**
- `id` - Primary key
- `organisationId` - Multi-tenant isolation
- `projectId` - Dataset belongs to project
- `jobId` - Generated by processing job
- `name` - User-defined dataset name
- `description` - Optional dataset notes
- `format` - Export format (csv, json, jsonl, parquet)
- `s3Key` - S3 object identifier
- `fileSize` - File size in bytes
- `recordCount` - Number of records in dataset
- `piiRedacted` - Privacy audit metric
- `checksumSha256` - SHA-256 hash for integrity
- `expiresAt` - Optional expiration timestamp (30-day retention)
- `deletedAt` - Soft delete timestamp
- `createdAt`, `updatedAt` - Audit columns

**Constraints:**
- FK CASCADE - delete dataset with parent entities
- NOT NULL on `name`, `format`, `s3Key`, `fileSize`, `recordCount`, `checksumSha256`

**Indexes:**
- `projectId` - for project's datasets query
- `jobId` - for job output lookup
- `organisationId` - for tenant queries
- `expiresAt` - for cleanup jobs (NULL-safe)

---

### 3. Integration & Security

#### oauth_connections

**Purpose:** Encrypted OAuth tokens for Teamwork Desk

```typescript
export const oauthConnections = pgTable('oauth_connections', {
  id: serial('id').primaryKey(),
  organisationId: integer('organisation_id').notNull().references(() => organisations.id, { onDelete: 'cascade' }),
  userId: integer('user_id').notNull().references(() => users.id, { onDelete: 'cascade' }),
  provider: text('provider').notNull(), // teamwork_desk
  providerUserId: text('provider_user_id'), // External user ID
  encryptedAccessToken: text('encrypted_access_token').notNull(), // AES-256-GCM encrypted
  encryptedRefreshToken: text('encrypted_refresh_token'), // Optional
  tokenExpiresAt: timestamp('token_expires_at'),
  scopes: text('scopes'), // Space-separated OAuth scopes
  status: text('status').notNull().default('active'), // active, expired, revoked
  lastSyncAt: timestamp('last_sync_at'), // Last successful API call
  createdAt: timestamp('created_at').defaultNow().notNull(),
  updatedAt: timestamp('updated_at').defaultNow().notNull(),
}, (table) => ({
  // One active connection per provider per organisation
  orgProviderUnique: unique('org_provider_unique').on(table.organisationId, table.provider),
}));

export type OAuthConnection = typeof oauthConnections.$inferSelect;
export type NewOAuthConnection = typeof oauthConnections.$inferInsert;
```

**Columns:**
- `id` - Primary key
- `organisationId` - Connection scoped to organisation
- `userId` - User who authorized connection
- `provider` - OAuth provider identifier (teamwork_desk)
- `providerUserId` - External user ID from provider
- `encryptedAccessToken` - AES-256-GCM encrypted access token (CRITICAL: never plaintext)
- `encryptedRefreshToken` - Encrypted refresh token (optional)
- `tokenExpiresAt` - Token expiration timestamp
- `scopes` - Granted OAuth scopes
- `status` - Connection state (active, expired, revoked)
- `lastSyncAt` - Last successful API call (for staleness detection)
- `createdAt`, `updatedAt` - Audit columns

**Encryption Requirements (per Architecture Section 13.3):**
- MUST encrypt `accessToken` before insert/update
- MUST decrypt only when making API calls
- MUST use ENCRYPTION_KEY from environment (64 hex chars)
- Encryption library: `server/lib/encryption.ts`

**Constraints:**
- UNIQUE (`organisationId`, `provider`) - one connection per provider per org
- FK CASCADE - delete connection with org or user
- NOT NULL on `provider`, `encryptedAccessToken`, `status`

**Indexes:**
- Composite unique on (`organisationId`, `provider`) - automatic via UNIQUE
- `status` - for connection health monitoring

---

### 4. Compliance & Audit

#### audit_logs

**Purpose:** Compliance and activity tracking

```typescript
export const auditLogs = pgTable('audit_logs', {
  id: serial('id').primaryKey(),
  organisationId: integer('organisation_id').notNull().references(() => organisations.id, { onDelete: 'cascade' }),
  userId: integer('user_id').references(() => users.id, { onDelete: 'set null' }), // null for system events
  action: text('action').notNull(), // user.login, project.create, job.process, dataset.export, etc.
  entityType: text('entity_type'), // project, data_source, job, dataset
  entityId: integer('entity_id'), // ID of affected entity
  metadata: text('metadata'), // JSON: IP address, user agent, details
  createdAt: timestamp('created_at').defaultNow().notNull(),
}, (table) => ({
  // Index for compliance queries
  orgActionIdx: index('audit_logs_org_action_idx').on(table.organisationId, table.action),
  // Index for entity audit trail
  entityIdx: index('audit_logs_entity_idx').on(table.entityType, table.entityId),
  // Index for time-based queries
  createdAtIdx: index('audit_logs_created_at_idx').on(table.createdAt),
}));

export type AuditLog = typeof auditLogs.$inferSelect;
export type NewAuditLog = typeof auditLogs.$inferInsert;
```

**Columns:**
- `id` - Primary key
- `organisationId` - Multi-tenant isolation
- `userId` - Actor (null for system events)
- `action` - Event type (user.login, project.create, dataset.export)
- `entityType` - Affected entity type (project, dataset, etc.)
- `entityId` - Affected entity ID
- `metadata` - JSON string with IP, user agent, additional context
- `createdAt` - Event timestamp (no updatedAt - immutable logs)

**Action Naming Convention:**
```
<entity>.<verb>
Examples:
  user.login, user.logout
  project.create, project.update, project.delete
  job.start, job.complete, job.fail
  dataset.export, dataset.delete
  oauth.connect, oauth.disconnect
```

**Constraints:**
- FK `organisationId` CASCADE - delete logs with org
- FK `userId` SET NULL - preserve logs if user deleted
- NOT NULL on `action`

**Indexes:**
- Composite (`organisationId`, `action`) - for compliance reports
- Composite (`entityType`, `entityId`) - for entity audit trail
- `createdAt` - for time-based queries

**Retention Policy:** 
- Logs retained indefinitely in MVP
- Post-MVP: consider archival strategy for logs >1 year old

---

## PHASE 5: INDEX STRATEGY

### Primary Key Indexes (Automatic)

All tables use `serial` primary keys, automatically indexed.

### Foreign Key Indexes (Required)

**Per Agent 3 guardrails:** All foreign keys must be explicitly indexed.

```typescript
// In each table definition, foreign key columns already have indexes via:
// - UNIQUE constraints (automatic index)
// - references() in Drizzle automatically creates FK constraint
// - Manual index() definitions for non-unique FKs
```

**Explicitly Required Indexes:**

| Table | Column | Rationale |
|-------|--------|-----------|
| users | organisationId | Multi-tenant queries |
| users | invitedBy | Invitation chain queries |
| team_members | organisationId | Role lookups |
| team_members | userId | User's organisations |
| projects | organisationId | Tenant project lists |
| projects | userId | User's projects |
| data_sources | projectId | Project sources |
| data_sources | organisationId | Tenant sources |
| schema_mappings | projectId | Project mappings |
| schema_mappings | dataSourceId | Unique constraint (automatic) |
| processing_jobs | projectId | Project job history |
| processing_jobs | dataSourceId | Source processing history |
| processing_jobs | organisationId | Tenant job monitoring |
| datasets | projectId | Project datasets |
| datasets | jobId | Job output lookup |
| datasets | organisationId | Tenant datasets |
| oauth_connections | organisationId | Org connections |
| oauth_connections | userId | User connections |
| audit_logs | organisationId | Compliance queries |
| audit_logs | userId | User activity tracking |

### Query-Driven Composite Indexes

Based on PRD user stories and expected access patterns:

```typescript
// projects: Active project listings
index('projects_org_deleted_idx').on(projects.organisationId, projects.deletedAt)
// Query: SELECT * FROM projects WHERE organisation_id = ? AND deleted_at IS NULL

// processing_jobs: Worker polling for pending jobs
index('processing_jobs_status_idx').on(processingJobs.status)
// Query: SELECT * FROM processing_jobs WHERE status = 'pending' LIMIT 1

// processing_jobs: Active jobs monitoring
index('processing_jobs_org_status_idx').on(processingJobs.organisationId, processingJobs.status)
// Query: SELECT * FROM processing_jobs WHERE organisation_id = ? AND status = 'processing'

// audit_logs: Compliance reports by action type
index('audit_logs_org_action_idx').on(auditLogs.organisationId, auditLogs.action)
// Query: SELECT * FROM audit_logs WHERE organisation_id = ? AND action LIKE 'dataset.%'

// audit_logs: Entity audit trail
index('audit_logs_entity_idx').on(auditLogs.entityType, auditLogs.entityId)
// Query: SELECT * FROM audit_logs WHERE entity_type = 'project' AND entity_id = ?

// audit_logs: Time-based queries
index('audit_logs_created_at_idx').on(auditLogs.createdAt)
// Query: SELECT * FROM audit_logs WHERE created_at > ?

// datasets: Expiration cleanup job
index('datasets_expires_at_idx').on(datasets.expiresAt)
// Query: SELECT * FROM datasets WHERE expires_at < NOW() AND deleted_at IS NULL
```

### Index Maintenance

- PostgreSQL automatically maintains indexes
- VACUUM operations scheduled via Replit managed DB
- Monitor index bloat post-MVP via pg_stat_user_indexes

---

## PHASE 6: CHECKPOINT 2

### Complete Schema Overview

**10 tables defined:**
1. organisations (3 FK references FROM users, projects, oauth_connections, team_members, data_sources, schema_mappings, processing_jobs, datasets, audit_logs)
2. users (2 FK references FROM team_members, projects)
3. team_members (junction table, 2 FKs TO organisations, users)
4. projects (4 FK references FROM data_sources, schema_mappings, processing_jobs, datasets)
5. data_sources (3 FK references FROM schema_mappings, processing_jobs)
6. schema_mappings (3 FKs TO organisations, projects, data_sources)
7. processing_jobs (4 FKs TO organisations, projects, data_sources; 1 FK reference FROM datasets)
8. datasets (3 FKs TO organisations, projects, processing_jobs)
9. oauth_connections (2 FKs TO organisations, users)
10. audit_logs (2 FKs TO organisations, users)

### Entity Relationship Diagram (Textual)

```
organisations (root)
 ├─── users (restrict delete)
 │     ├─── projects (restrict delete)
 │     │     ├─── data_sources (cascade)
 │     │     │     ├─── schema_mappings (cascade)
 │     │     │     └─── processing_jobs (cascade)
 │     │     │           └─── datasets (cascade)
 │     │     ├─── schema_mappings (cascade)
 │     │     ├─── processing_jobs (cascade)
 │     │     └─── datasets (cascade)
 │     ├─── team_members (cascade)
 │     ├─── oauth_connections (cascade)
 │     └─── audit_logs (set null)
 ├─── projects (cascade)
 ├─── data_sources (cascade)
 ├─── schema_mappings (cascade)
 ├─── processing_jobs (cascade)
 ├─── datasets (cascade)
 ├─── oauth_connections (cascade)
 ├─── team_members (cascade)
 └─── audit_logs (cascade)
```

### Cascade Behavior Summary

| Parent → Child | Behavior | Rationale |
|----------------|----------|-----------|
| organisations → * (all children) | CASCADE | Delete all org data when org deleted |
| users → projects | RESTRICT | Preserve projects, require reassignment |
| users → team_members | CASCADE | Remove membership when user deleted |
| users → oauth_connections | CASCADE | Remove connections when user deleted |
| users → audit_logs | SET NULL | Preserve audit trail, show "deleted user" |
| users.invitedBy → users | SET NULL | Preserve user if inviter deleted |
| projects → data_sources | CASCADE | Delete sources when project deleted |
| projects → processing_jobs | CASCADE | Delete jobs when project deleted |
| projects → datasets | CASCADE | Delete datasets when project deleted |
| data_sources → processing_jobs | CASCADE | Delete jobs when source deleted |
| processing_jobs → datasets | CASCADE | Delete datasets when job deleted |

### Type Exports Strategy

All tables export two types:
- `EntityName` - Full record type (for reads)
- `NewEntityName` - Insert type (for creates, omits auto-generated fields)

**Continuing to migration strategy...**

---

## PHASE 7: MIGRATION STRATEGY

### Initial Migration Approach

**Single Migration File:** MVP uses one migration to create all tables in correct dependency order.

**Migration Order:**
1. organisations (no dependencies)
2. users (depends on organisations)
3. team_members (depends on organisations, users)
4. projects (depends on organisations, users)
5. data_sources (depends on organisations, projects)
6. schema_mappings (depends on organisations, projects, data_sources)
7. processing_jobs (depends on organisations, projects, data_sources)
8. datasets (depends on organisations, projects, processing_jobs)
9. oauth_connections (depends on organisations, users)
10. audit_logs (depends on organisations, users)

### Drizzle Configuration

```typescript
// drizzle.config.ts
import type { Config } from 'drizzle-kit';
import * as dotenv from 'dotenv';

dotenv.config();

export default {
  schema: './server/db/schema.ts',
  out: './server/db/migrations',
  driver: 'pg',
  dbCredentials: {
    connectionString: process.env.DATABASE_URL!,
  },
} satisfies Config;
```

### Migration Commands

```bash
# Generate migration from schema changes
npm run db:generate

# Apply migrations to database
npm run db:migrate

# Push schema directly (development only, skips migration generation)
npm run db:push
```

**package.json scripts:**
```json
{
  "scripts": {
    "db:generate": "drizzle-kit generate:pg",
    "db:migrate": "tsx server/db/migrate.ts",
    "db:push": "drizzle-kit push:pg",
    "db:studio": "drizzle-kit studio",
    "db:seed": "tsx server/db/seed.ts"
  }
}
```

### Migration Script

```typescript
// server/db/migrate.ts
import { drizzle } from 'drizzle-orm/postgres-js';
import { migrate } from 'drizzle-orm/postgres-js/migrator';
import postgres from 'postgres';
import { env } from '../config/env';

async function runMigrations() {
  console.log('Running database migrations...');
  
  const sql = postgres(env.DATABASE_URL, { max: 1 });
  const db = drizzle(sql);
  
  await migrate(db, { migrationsFolder: './server/db/migrations' });
  
  await sql.end();
  console.log('Migrations complete');
}

runMigrations().catch((err) => {
  console.error('Migration failed:', err);
  process.exit(1);
});
```

### Seed Data Strategy

**Purpose:** Provide test data for development and demo environments.

**Seed Data Requirements:**

1. **Test Organisation:**
   - Name: "Acme Corporation"
   - Slug: "acme-corp"
   - Subscription: "pro"

2. **Test Users:**
   - Admin user: admin@acme-corp.com (password: "password123")
   - Regular user: user@acme-corp.com (password: "password123")
   - Viewer user: viewer@acme-corp.com (password: "password123")

3. **Team Members:**
   - Admin user with "admin" role
   - Regular user with "editor" role
   - Viewer user with "viewer" role

4. **Sample Project:**
   - Name: "Support Ticket Analysis"
   - Description: "Training data for AI support agent"
   - Created by admin user

5. **Sample Data Source:**
   - Name: "Q1 Support Tickets"
   - Type: "file"
   - Format: "csv"
   - No actual S3 file (development only)

### Seed Script

```typescript
// server/db/seed.ts
import { db } from './index';
import { organisations, users, teamMembers, projects, dataSources } from './schema';
import bcrypt from 'bcryptjs';

async function seed() {
  console.log('Seeding database...');
  
  // 1. Create test organisation
  const [org] = await db
    .insert(organisations)
    .values({
      name: 'Acme Corporation',
      slug: 'acme-corp',
      subscriptionTier: 'pro',
      subscriptionStatus: 'active',
    })
    .returning();
  
  // 2. Create test users
  const [adminUser] = await db
    .insert(users)
    .values({
      organisationId: org.id,
      email: 'admin@acme-corp.com',
      passwordHash: await bcrypt.hash('password123', 10),
      name: 'Admin User',
      role: 'admin',
      status: 'active',
    })
    .returning();
  
  const [editorUser] = await db
    .insert(users)
    .values({
      organisationId: org.id,
      email: 'editor@acme-corp.com',
      passwordHash: await bcrypt.hash('password123', 10),
      name: 'Editor User',
      role: 'user',
      status: 'active',
    })
    .returning();
  
  const [viewerUser] = await db
    .insert(users)
    .values({
      organisationId: org.id,
      email: 'viewer@acme-corp.com',
      passwordHash: await bcrypt.hash('password123', 10),
      name: 'Viewer User',
      role: 'user',
      status: 'active',
    })
    .returning();
  
  // 3. Create team memberships
  await db.insert(teamMembers).values([
    { organisationId: org.id, userId: adminUser.id, role: 'admin' },
    { organisationId: org.id, userId: editorUser.id, role: 'editor' },
    { organisationId: org.id, userId: viewerUser.id, role: 'viewer' },
  ]);
  
  // 4. Create sample project
  const [project] = await db
    .insert(projects)
    .values({
      organisationId: org.id,
      userId: adminUser.id,
      name: 'Support Ticket Analysis',
      description: 'Training data for AI support agent',
      canonicalSchema: 'conversation',
    })
    .returning();
  
  // 5. Create sample data source
  await db
    .insert(dataSources)
    .values({
      organisationId: org.id,
      projectId: project.id,
      name: 'Q1 Support Tickets',
      type: 'file',
      fileFormat: 'csv',
      recordCount: 1500,
      metadata: JSON.stringify({ 
        originalFilename: 'support_tickets_q1_2026.csv',
        uploadedAt: new Date().toISOString(),
      }),
    });
  
  console.log('Seed complete');
  console.log('Test credentials:');
  console.log('  Admin: admin@acme-corp.com / password123');
  console.log('  Editor: editor@acme-corp.com / password123');
  console.log('  Viewer: viewer@acme-corp.com / password123');
}

seed().catch((err) => {
  console.error('Seed failed:', err);
  process.exit(1);
});
```

### Rollback Strategy

**Drizzle Limitations:** Drizzle ORM does not generate automatic rollback migrations.

**Rollback Approach:**
1. Manual SQL rollback scripts for production
2. Development: `npm run db:push` to force schema state
3. Critical: Backup database before production migrations

**Example Manual Rollback:**
```sql
-- migrations/rollback_001_initial_schema.sql
DROP TABLE IF EXISTS audit_logs CASCADE;
DROP TABLE IF EXISTS oauth_connections CASCADE;
DROP TABLE IF EXISTS datasets CASCADE;
DROP TABLE IF EXISTS processing_jobs CASCADE;
DROP TABLE IF EXISTS schema_mappings CASCADE;
DROP TABLE IF EXISTS data_sources CASCADE;
DROP TABLE IF EXISTS projects CASCADE;
DROP TABLE IF EXISTS team_members CASCADE;
DROP TABLE IF EXISTS users CASCADE;
DROP TABLE IF EXISTS organisations CASCADE;
```

---

## PHASE 8: VALIDATION

### PRD Entity Coverage

| PRD Entity | Schema Table | Status |
|------------|--------------|--------|
| Users | users | ✓ Complete |
| Organisations | organisations | ✓ Complete |
| Teams/Collaboration | team_members | ✓ Complete |
| Projects | projects | ✓ Complete |
| Data Sources (files, APIs) | data_sources | ✓ Complete |
| Schema Mappings | schema_mappings | ✓ Complete |
| Processing Jobs | processing_jobs | ✓ Complete |
| Datasets/Exports | datasets | ✓ Complete |
| OAuth Connections | oauth_connections | ✓ Complete |
| Audit Logs | audit_logs | ✓ Complete |

**All PRD entities represented: YES**

### Architecture Compatibility

| Constraint | Implementation | Status |
|------------|----------------|--------|
| PostgreSQL via postgres-js | ✓ Connection config uses postgres-js | ✓ |
| Drizzle ORM Core Select API | ✓ Schema uses pgTable | ✓ |
| Replit deployment | ✓ Connection pooling configured | ✓ |
| Multi-tenancy | ✓ organisationId on all tenant tables | ✓ |
| Encryption for OAuth tokens | ✓ encryptedAccessToken column | ✓ |
| Soft delete | ✓ deletedAt on projects, data_sources, datasets | ✓ |
| Audit columns | ✓ createdAt, updatedAt on all tables | ✓ |

**Architecture compatibility: YES**

### Drizzle Standards Compliance

| Standard | Implementation | Status |
|----------|----------------|--------|
| postgres-js driver | ✓ Specified in connection config | ✓ |
| Core Select API only | ✓ No Query API usage | ✓ |
| Type exports | ✓ All tables export types | ✓ |
| Audit columns | ✓ createdAt, updatedAt standard | ✓ |
| Foreign key indexes | ✓ All FKs indexed | ✓ |
| Cascade behavior | ✓ All FKs specify onDelete | ✓ |

**Standards compliance: YES**

---

## PHASE 9: CHECKPOINT 3 - COMPLETE SCHEMA

### Complete Schema File

```typescript
// server/db/schema.ts
import { pgTable, serial, text, timestamp, integer, boolean, unique, index } from 'drizzle-orm/pg-core';

// ============================================================
// AUDIT COLUMNS HELPER
// ============================================================

const auditColumns = {
  createdAt: timestamp('created_at').defaultNow().notNull(),
  updatedAt: timestamp('updated_at').defaultNow().notNull(),
};

// ============================================================
// 1. CORE AUTHENTICATION & MULTI-TENANCY
// ============================================================

export const organisations = pgTable('organisations', {
  id: serial('id').primaryKey(),
  name: text('name').notNull(),
  slug: text('slug').notNull().unique(),
  subscriptionTier: text('subscription_tier').notNull().default('free'),
  subscriptionStatus: text('subscription_status').notNull().default('active'),
  ...auditColumns,
});

export const users = pgTable('users', {
  id: serial('id').primaryKey(),
  organisationId: integer('organisation_id').notNull().references(() => organisations.id, { onDelete: 'restrict' }),
  email: text('email').notNull().unique(),
  passwordHash: text('password_hash').notNull(),
  name: text('name'),
  role: text('role').notNull().default('user'),
  status: text('status').notNull().default('active'),
  invitedBy: integer('invited_by').references(() => users.id, { onDelete: 'set null' }),
  lastLoginAt: timestamp('last_login_at'),
  ...auditColumns,
});

export const teamMembers = pgTable('team_members', {
  id: serial('id').primaryKey(),
  organisationId: integer('organisation_id').notNull().references(() => organisations.id, { onDelete: 'cascade' }),
  userId: integer('user_id').notNull().references(() => users.id, { onDelete: 'cascade' }),
  role: text('role').notNull(),
  ...auditColumns,
}, (table) => ({
  orgUserUnique: unique('org_user_unique').on(table.organisationId, table.userId),
}));

// ============================================================
// 2. DATA PREPARATION WORKFLOW
// ============================================================

export const projects = pgTable('projects', {
  id: serial('id').primaryKey(),
  organisationId: integer('organisation_id').notNull().references(() => organisations.id, { onDelete: 'cascade' }),
  userId: integer('user_id').notNull().references(() => users.id, { onDelete: 'restrict' }),
  name: text('name').notNull(),
  description: text('description'),
  canonicalSchema: text('canonical_schema').notNull().default('conversation'),
  deletedAt: timestamp('deleted_at'),
  ...auditColumns,
}, (table) => ({
  orgDeletedIdx: index('projects_org_deleted_idx').on(table.organisationId, table.deletedAt),
}));

export const dataSources = pgTable('data_sources', {
  id: serial('id').primaryKey(),
  organisationId: integer('organisation_id').notNull().references(() => organisations.id, { onDelete: 'cascade' }),
  projectId: integer('project_id').notNull().references(() => projects.id, { onDelete: 'cascade' }),
  name: text('name').notNull(),
  type: text('type').notNull(),
  fileFormat: text('file_format'),
  s3Key: text('s3_key'),
  fileSize: integer('file_size'),
  recordCount: integer('record_count'),
  metadata: text('metadata'),
  deletedAt: timestamp('deleted_at'),
  ...auditColumns,
});

export const schemaMappings = pgTable('schema_mappings', {
  id: serial('id').primaryKey(),
  organisationId: integer('organisation_id').notNull().references(() => organisations.id, { onDelete: 'cascade' }),
  projectId: integer('project_id').notNull().references(() => projects.id, { onDelete: 'cascade' }),
  dataSourceId: integer('data_source_id').notNull().references(() => dataSources.id, { onDelete: 'cascade' }),
  mappingConfig: text('mapping_config').notNull(),
  validatedAt: timestamp('validated_at'),
  ...auditColumns,
}, (table) => ({
  dataSourceUnique: unique('data_source_unique').on(table.dataSourceId),
}));

export const processingJobs = pgTable('processing_jobs', {
  id: serial('id').primaryKey(),
  organisationId: integer('organisation_id').notNull().references(() => organisations.id, { onDelete: 'cascade' }),
  projectId: integer('project_id').notNull().references(() => projects.id, { onDelete: 'cascade' }),
  dataSourceId: integer('data_source_id').notNull().references(() => dataSources.id, { onDelete: 'cascade' }),
  status: text('status').notNull().default('pending'),
  progress: integer('progress').notNull().default(0),
  recordsProcessed: integer('records_processed').notNull().default(0),
  recordsTotal: integer('records_total').notNull().default(0),
  piiDetected: integer('pii_detected').notNull().default(0),
  errorMessage: text('error_message'),
  startedAt: timestamp('started_at'),
  completedAt: timestamp('completed_at'),
  ...auditColumns,
}, (table) => ({
  statusIdx: index('processing_jobs_status_idx').on(table.status),
  orgStatusIdx: index('processing_jobs_org_status_idx').on(table.organisationId, table.status),
}));

export const datasets = pgTable('datasets', {
  id: serial('id').primaryKey(),
  organisationId: integer('organisation_id').notNull().references(() => organisations.id, { onDelete: 'cascade' }),
  projectId: integer('project_id').notNull().references(() => projects.id, { onDelete: 'cascade' }),
  jobId: integer('job_id').notNull().references(() => processingJobs.id, { onDelete: 'cascade' }),
  name: text('name').notNull(),
  description: text('description'),
  format: text('format').notNull(),
  s3Key: text('s3_key').notNull(),
  fileSize: integer('file_size').notNull(),
  recordCount: integer('record_count').notNull(),
  piiRedacted: integer('pii_redacted').notNull().default(0),
  checksumSha256: text('checksum_sha256').notNull(),
  expiresAt: timestamp('expires_at'),
  deletedAt: timestamp('deleted_at'),
  ...auditColumns,
});

// ============================================================
// 3. INTEGRATION & SECURITY
// ============================================================

export const oauthConnections = pgTable('oauth_connections', {
  id: serial('id').primaryKey(),
  organisationId: integer('organisation_id').notNull().references(() => organisations.id, { onDelete: 'cascade' }),
  userId: integer('user_id').notNull().references(() => users.id, { onDelete: 'cascade' }),
  provider: text('provider').notNull(),
  providerUserId: text('provider_user_id'),
  encryptedAccessToken: text('encrypted_access_token').notNull(),
  encryptedRefreshToken: text('encrypted_refresh_token'),
  tokenExpiresAt: timestamp('token_expires_at'),
  scopes: text('scopes'),
  status: text('status').notNull().default('active'),
  lastSyncAt: timestamp('last_sync_at'),
  ...auditColumns,
}, (table) => ({
  orgProviderUnique: unique('org_provider_unique').on(table.organisationId, table.provider),
}));

// ============================================================
// 4. COMPLIANCE & AUDIT
// ============================================================

export const auditLogs = pgTable('audit_logs', {
  id: serial('id').primaryKey(),
  organisationId: integer('organisation_id').notNull().references(() => organisations.id, { onDelete: 'cascade' }),
  userId: integer('user_id').references(() => users.id, { onDelete: 'set null' }),
  action: text('action').notNull(),
  entityType: text('entity_type'),
  entityId: integer('entity_id'),
  metadata: text('metadata'),
  createdAt: timestamp('created_at').defaultNow().notNull(),
}, (table) => ({
  orgActionIdx: index('audit_logs_org_action_idx').on(table.organisationId, table.action),
  entityIdx: index('audit_logs_entity_idx').on(table.entityType, table.entityId),
  createdAtIdx: index('audit_logs_created_at_idx').on(table.createdAt),
}));

// ============================================================
// TYPE EXPORTS
// ============================================================

export type Organisation = typeof organisations.$inferSelect;
export type NewOrganisation = typeof organisations.$inferInsert;

export type User = typeof users.$inferSelect;
export type NewUser = typeof users.$inferInsert;

export type TeamMember = typeof teamMembers.$inferSelect;
export type NewTeamMember = typeof teamMembers.$inferInsert;

export type Project = typeof projects.$inferSelect;
export type NewProject = typeof projects.$inferInsert;

export type DataSource = typeof dataSources.$inferSelect;
export type NewDataSource = typeof dataSources.$inferInsert;

export type SchemaMapping = typeof schemaMappings.$inferSelect;
export type NewSchemaMapping = typeof schemaMappings.$inferInsert;

export type ProcessingJob = typeof processingJobs.$inferSelect;
export type NewProcessingJob = typeof processingJobs.$inferInsert;

export type Dataset = typeof datasets.$inferSelect;
export type NewDataset = typeof datasets.$inferInsert;

export type OAuthConnection = typeof oauthConnections.$inferSelect;
export type NewOAuthConnection = typeof oauthConnections.$inferInsert;

export type AuditLog = typeof auditLogs.$inferSelect;
export type NewAuditLog = typeof auditLogs.$inferInsert;
```

### Connection Configuration

```typescript
// server/db/index.ts
import postgres from 'postgres';
import { drizzle } from 'drizzle-orm/postgres-js';
import * as schema from './schema';
import { env } from '../config/env';

const sql = postgres(env.DATABASE_URL, {
  max: 10,
  idle_timeout: 20,
  connect_timeout: 10,
});

export const db = drizzle(sql, { schema });

export async function closeDatabase() {
  await sql.end();
}

// Graceful shutdown
process.on('SIGTERM', async () => {
  console.log('SIGTERM received, closing database connection...');
  await closeDatabase();
  process.exit(0);
});
```

### Query Pattern Examples

```typescript
// server/routes/projects.ts
import { db } from '../db';
import { projects, users } from '../db/schema';
import { eq, and, isNull } from 'drizzle-orm';

// ✓ CORRECT: Core Select API with explicit joins
export async function getActiveProjects(organisationId: number) {
  return await db
    .select({
      project: projects,
      user: users,
    })
    .from(projects)
    .leftJoin(users, eq(projects.userId, users.id))
    .where(
      and(
        eq(projects.organisationId, organisationId),
        isNull(projects.deletedAt)
      )
    )
    .orderBy(projects.createdAt);
}

// ✓ CORRECT: Single record query
export async function getProject(id: number) {
  const [project] = await db
    .select()
    .from(projects)
    .where(eq(projects.id, id))
    .limit(1);
  
  return project;
}

// ✓ CORRECT: Transaction pattern
export async function createProjectWithSource(projectData, sourceData) {
  return await db.transaction(async (tx) => {
    const [project] = await tx
      .insert(projects)
      .values(projectData)
      .returning();
    
    const [source] = await tx
      .insert(dataSources)
      .values({ ...sourceData, projectId: project.id })
      .returning();
    
    return { project, source };
  });
}

// ✓ CORRECT: N+1 prevention with aggregation
import { count } from 'drizzle-orm';

export async function getProjectsWithSourceCounts(organisationId: number) {
  return await db
    .select({
      project: projects,
      sourceCount: count(dataSources.id),
    })
    .from(projects)
    .leftJoin(dataSources, eq(dataSources.projectId, projects.id))
    .where(
      and(
        eq(projects.organisationId, organisationId),
        isNull(projects.deletedAt)
      )
    )
    .groupBy(projects.id);
}

// ✓ CORRECT: Soft delete
export async function deleteProject(id: number) {
  await db
    .update(projects)
    .set({ deletedAt: new Date() })
    .where(eq(projects.id, id));
}

// ✗ FORBIDDEN: Query API (inconsistent behavior)
// const project = await db.query.projects.findFirst({
//   where: eq(projects.id, id),
// });
```

---

## Document Validation

### Completeness Checklist

- [x] All PRD entities represented
- [x] All relationships defined with cascade behavior
- [x] Foreign keys indexed
- [x] Audit columns on all tables
- [x] Migration scripts specified
- [x] Type exports defined
- [x] Drizzle ORM patterns documented
- [x] Query examples provided
- [x] Seed data strategy defined

### Prompt Maintenance Contract

If this prompt is edited, you MUST:
1. Update version history with changes and `Hygiene Gate: PASS`
2. Re-run Prompt Hygiene Gate checks (Constitution Section L)
3. Confirm clean encoding (no mojibake/non-ASCII artifacts)
4. Verify no global rule restatements (reference Constitution instead)

Failed checks invalidate prompt update.

### Prompt Hygiene Gate (Constitution Section L)

- [x] Framework Version header present and correct
- [x] Encoding scan: No non-ASCII artifact tokens
- [x] Inheritance references Constitution v3.3
- [x] No full global rule restatements (uses "Per Constitution Section X")

### Entity Coverage

| PRD Entity | Schema Table | Status |
|------------|--------------|--------|
| Users | users | ✓ Complete |
| Organisations | organisations | ✓ Complete |
| Teams/Collaboration | team_members | ✓ Complete |
| Projects | projects | ✓ Complete |
| Data Sources (files, APIs) | data_sources | ✓ Complete |
| Schema Mappings | schema_mappings | ✓ Complete |
| Processing Jobs | processing_jobs | ✓ Complete |
| Datasets/Exports | datasets | ✓ Complete |
| OAuth Connections | oauth_connections | ✓ Complete |
| Audit Logs | audit_logs | ✓ Complete |

### Document Status: COMPLETE

---

## Downstream Agent Handoff Brief

### Agent 4: API Contract

**Entity operations needed:**
- organisations: Read (single)
- users: Create, Read (single, list), Update (profile, password)
- team_members: Create, Read (list), Update (role), Delete
- projects: Create, Read (single, list), Update, Soft Delete
- data_sources: Create, Read (single, list), Soft Delete
- schema_mappings: Create, Read (single), Update
- processing_jobs: Create, Read (single, list), Update (progress, status)
- datasets: Read (single, list), Soft Delete, Download
- oauth_connections: Create, Read (single), Update (refresh token), Delete
- audit_logs: Read (list with filters)

**Relationship traversal patterns:**
- GET /projects/{id}/data-sources - project's sources
- GET /projects/{id}/jobs - project's processing jobs
- GET /projects/{id}/datasets - project's datasets
- GET /jobs/{id}/dataset - job's output dataset
- GET /organisations/{id}/members - organisation team members

**Pagination requirements:**
- projects, data_sources, processing_jobs, datasets, audit_logs
- Default limit: 50, max: 100
- Cursor-based pagination for audit_logs (createdAt)

### Agent 5: UI/UX Specification

**Data shapes for forms:**
- Project creation: name, description, canonicalSchema
- Data source upload: name, file, type
- Schema mapping: visual drag-drop (mappingConfig JSON)
- Team member invitation: email, role
- OAuth connection: provider, redirectUri

**List/detail patterns:**
- Projects list: name, description, source count, last updated, creator
- Data sources list: name, type, format, record count, upload date
- Processing jobs list: status, progress, records processed/total, started/completed
- Datasets list: name, format, record count, file size, created date, expiration

**Relationship displays:**
- Project detail: show data sources, active jobs, datasets
- Job detail: show source, mapping, progress, errors, output dataset
- Dataset detail: show source job, download options, PII redaction count

### Agent 6: Implementation Orchestrator

**Schema file location:** `server/db/schema.ts`

**Migration commands:**
```bash
npm run db:generate  # Generate migration
npm run db:migrate   # Apply migrations
npm run db:push      # Dev only: force schema state
npm run db:seed      # Populate test data
```

**Connection module:** `server/db/index.ts`

**Type imports:**
```typescript
import { 
  User, NewUser, 
  Project, NewProject,
  DataSource, NewDataSource,
  ProcessingJob, NewProcessingJob,
  Dataset, NewDataset,
} from './db/schema';
```

**Critical Query Patterns:**
- ALWAYS use Core Select API (`db.select().from()`)
- NEVER use Query API (`db.query.`)
- Use JOINs to prevent N+1 queries
- Use transactions for multi-table operations
- Apply soft delete filters: `isNull(projects.deletedAt)`

**Required Helper Functions:**
```typescript
// server/lib/queries.ts
export async function getActiveProjects(organisationId: number);
export async function getUserOrganisation(userId: number);
export async function getPendingJobs(limit: number);
export async function createAuditLog(action: string, userId: number, metadata: object);
```

### Agent 7: QA & Deployment

**Seed data for testing:**
- Run `npm run db:seed` before tests
- Test organisation: "Acme Corporation" (ID will vary)
- Test users: admin@acme-corp.com, editor@acme-corp.com, viewer@acme-corp.com
- All passwords: "password123"

**Migration verification steps:**
1. Check migration generated: `server/db/migrations/*.sql`
2. Verify all tables created: `\dt` in psql
3. Verify indexes: `\di` in psql
4. Check foreign keys: `\d table_name` in psql
5. Test seed data: `SELECT COUNT(*) FROM organisations;`

**Database health checks:**
```typescript
// Health endpoint
export async function checkDatabase() {
  try {
    await db.select().from(organisations).limit(1);
    return { status: 'healthy' };
  } catch (error) {
    return { status: 'unhealthy', error: error.message };
  }
}
```

**Common Issues:**
- Migration fails: Check dependency order (organisations before users, etc.)
- Connection pool exhausted: Verify `max: 10` in connection config
- Foreign key violations: Check cascade behavior on deletes
- Index missing: Verify foreign key indexes created
- Seed fails: Check unique constraints on email/slug

---

## ASSUMPTION REGISTER

### AR-DATA-001: Single Organisation per User (MVP)

- **Type:** ASSUMPTION
- **Source Gap:** PRD doesn't explicitly state whether users can belong to multiple organisations
- **Assumption Made:** MVP: Users belong to exactly one organisation via `organisationId` column on `users` table. No junction table `users_organisations`.
- **Impact if Wrong:** Need junction table (users_organisations), auth context must track "current organisation", UI needs org switcher, queries must filter by selected org (4-8 hours additional development)
- **Proposed Resolution:** Confirm with stakeholder: "Can a single user account belong to multiple organisations?" If yes, refactor to many-to-many relationship post-MVP.
- **Status:** UNRESOLVED (requires human confirmation)
- **Owner:** Human (Product stakeholder) → Agent 3 (Data Modeling if changes needed)
- **Date:** 2026-01-21

### AR-DATA-002: Soft Delete for User-Generated Content

- **Type:** ASSUMPTION
- **Source Gap:** PRD mentions data lifecycle but doesn't specify hard vs. soft delete strategy
- **Assumption Made:** Soft delete pattern (`deletedAt` timestamp) applied to `projects`, `data_sources`, `datasets` to enable recovery and audit trail per PRD compliance requirements
- **Impact if Wrong:** If hard delete required, remove `deletedAt` columns and simplify queries. Lose recovery capability and audit trail.
- **Proposed Resolution:** Confirm with stakeholder: "Do users need ability to restore deleted projects/datasets?" If no, simplify to hard deletes.
- **Status:** ACCEPTED (soft delete enables compliance audit trail)
- **Owner:** Agent 3 (Data Modeling)
- **Date:** 2026-01-21

### AR-DATA-003: Processing Jobs State Machine (No Partial Completion)

- **Type:** ASSUMPTION
- **Source Gap:** Architecture specifies 30-minute timeout but doesn't define behavior for partial results
- **Assumption Made:** Jobs have five states: pending, processing, completed, failed, cancelled. No "partially completed" state in MVP. Jobs exceeding timeout marked "failed" with `errorMessage`.
- **Impact if Wrong:** Users lose partial results if job times out. Need additional state and dataset versioning.
- **Proposed Resolution:** Instrument timeout frequency during beta. If >5% of jobs timeout, add "partial" state and save progress post-MVP.
- **Status:** ACCEPTED (timeout strategy per Architecture AR-006)
- **Owner:** Agent 7 (QA to monitor timeout frequency)
- **Date:** 2026-01-21

### AR-DATA-004: Metadata Columns as JSON Strings

- **Type:** ASSUMPTION
- **Source Gap:** PRD mentions "flexible metadata" but doesn't specify JSON vs. JSONB vs. separate columns
- **Assumption Made:** Metadata stored as JSON strings (`text` type) rather than PostgreSQL JSONB for simplicity. Examples: `data_sources.metadata`, `audit_logs.metadata`, `schema_mappings.mappingConfig`.
- **Impact if Wrong:** Cannot query inside JSON fields efficiently. Need migration to JSONB for complex queries.
- **Proposed Resolution:** If post-MVP requires JSON field queries (e.g., "find all sources uploaded by filename pattern"), migrate to JSONB and add GIN indexes.
- **Status:** ACCEPTED (text JSON sufficient for MVP, JSONB deferred)
- **Owner:** Agent 3 (Data Modeling) → Agent 6 (Implementation if JSONB needed)
- **Date:** 2026-01-21

### AR-DATA-005: Dataset Expiration Optional

- **Type:** ASSUMPTION
- **Source Gap:** Architecture mentions "30-day retention" for exports but doesn't mandate expiration enforcement
- **Assumption Made:** `datasets.expiresAt` is optional (nullable). If set, indicates target expiration date. Cleanup job required post-MVP to enforce.
- **Impact if Wrong:** Unlimited storage costs if datasets never expire. Need mandatory expiration or storage quotas.
- **Proposed Resolution:** Monitor storage costs during beta. If costs excessive, add automated cleanup job and make `expiresAt` mandatory (default 30 days).
- **Status:** ACCEPTED (optional expiration for MVP, cleanup job deferred)
- **Owner:** Agent 6 (Implementation if cleanup job needed)
- **Date:** 2026-01-21

---

## Document End
